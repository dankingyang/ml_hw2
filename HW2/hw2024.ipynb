{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnokyRPqBJ7n"
      },
      "source": [
        "# **Homework - Spring 2024**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXCMO-KSHept"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.linalg import *\n",
        "np.random.seed(42)  # don't change this line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppXdHvskhkLV"
      },
      "source": [
        "## **STUDENT ID Setup**\n",
        "Enter your 10-digit ID below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7nuuZCohkLW"
      },
      "outputs": [],
      "source": [
        "STUDENT_ID = 1          # YOUR STUDENT-ID GOES HERE AS AN INTEGER#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **1. Linear Regression [29pts]**\n",
        "\n",
        "## **1.1. Linear Regression Implementation [19 pts]**\n",
        "\n",
        "In this section you will implement linear regression with both L1 and L2 regularization. Your class LinearRegression must implement the following API:\n",
        "\n",
        "* `__init__(alpha, tol, max_iter, theta_init, penalty, lambd)`\n",
        "* `compute_cost(theta, X, y)`\n",
        "* `compute_gradient(theta, X, y)`\n",
        "* `fit(X, y)`\n",
        "* `has_converged(theta_old, theta_new)`\n",
        "* `predict(X)`\n",
        "\n",
        "Note that these methods have already been defined correctly for you in the LinearRegression class. **DO NOT** change the API.\n",
        "\n",
        "### **1.1.1. Cost Function [5 pts]**\n",
        "\n",
        "The `compute_cost` function should compute the cost for a given $\\theta$ vector. The cost is a scalar value given by:\n",
        "\n",
        "$\n",
        "\\mathcal{L}({\\theta}) = \\frac{1}{N}\\sum_{i =1}^N (h_{{\\theta}}({x}_i) - y_i)^2\n",
        "$\n",
        "\n",
        "where\n",
        "\n",
        "> $h_{{\\theta}}({x}_i) = \\theta^Tx_i$\n",
        "\n",
        "Based on the regularization penalty, you may need to add below regularization penalty loss to MSE Loss computed previously.\n",
        "\n",
        "L1 Regularization Loss:\n",
        ">$\n",
        "\\mathcal{L_1}({\\theta}) = \\mathcal{L}({\\theta}) + \\lambda\\sum_{j = 1}^D  |{\\theta}_j|\n",
        "$\n",
        "\n",
        "L2 Regularization Loss:\n",
        ">$\n",
        "\\mathcal{L_2}({\\theta}) = \\mathcal{L}({\\theta}) + \\lambda\\sum_{j = 1}^D  {\\theta}_j^2 \n",
        "$\n",
        "\n",
        "$N$ is the number of training samples and $D$ is the number of features (excluding the intercept term). $\\theta$ is a $D + 1$ dimensional vector, with the first element being the intercept term. <font color=Red>Note that we do not include the intercept in the regularization terms.</font>\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.2. Gradient of the Cost Function [5 pts]**\n",
        "\n",
        "The `compute_gradient` function should compute the gradient of the cost function at a given $\\theta$.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.3. Convergence Check [1 pt]**\n",
        "\n",
        "The `has_converged` function should return whether gradient descent algorithm has converged or not. Refer 1.1.4 for convergence condition.\n",
        " \n",
        "---\n",
        "\n",
        "### **1.1.4. Training with Gradient Descent [3 pts]**\n",
        "\n",
        "The `fit` method should train the model via gradient descent, relying on the `compute_cost` and `compute_gradient` functions. The trained weights/coefficients must be stored as `theta_`. The weights and the corresponding cost after every gradient descent iteration must be stored in `hist_theta_` and `hist_cost_` respectively.\n",
        "\n",
        "* The gradient descent stops or converges when $\\theta$ stops changing or changes negligibly between consecutive iterations, i.e., when \n",
        "$\\| {\\theta}_\\mathit{new} -  {\\theta}_\\mathit{old} \\|_2 \\leq \\epsilon$, \n",
        "for some small $\\epsilon$ (e.g., $\\epsilon$ = 1E-4). $\\epsilon$ is stored as `tol` (short for tolerance). \n",
        "\n",
        "* To ensure that the function terminates, we should set a maximum limit for the number of epochs irrespective of whether $\\theta$ converges or not. The limit is stored as `max_iter`.\n",
        "\n",
        "* `alpha` is the learning rate of the gradient descent algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.5. Training with Stochastic Gradient Descent (SGD) [3 pts]**\n",
        "\n",
        "The `fit_sgd` method should train the model via stochastic gradient descent (SGD), relying on the `compute_cost` and `compute_gradient` functions.\n",
        "\n",
        "The trained weights/coefficients must be stored as `theta_`. The weights and the corresponding cost after every SGD iteration must be stored in `hist_theta_` and `hist_cost_` respectively.\n",
        "\n",
        "* Unlike regular (or batch) gradient descent, SGD takes a gradient step on a single training example on each iteration. In other words, rather than compute the gradient for all training examples, summing them, and taking a single gradient step, it iterates through training examples, computes the gradient for that training example, and immediately takes a single gradient step before proceeding to the next training example. One pass over the entire training dataset is called an epoch; at the end of an epoch, the next epoch restarts from the first example in the training dataset.\n",
        "\n",
        "* As with gradient descent, SGD stops or converges when $\\theta$ stops changing or changes negligibly between consecutive iterations, i.e., when \n",
        "$\\| {\\theta}_\\mathit{new} -  {\\theta}_\\mathit{old} \\|_2 \\leq \\epsilon$, \n",
        "for some small $\\epsilon$ (e.g., $\\epsilon$ = 1E-4). $\\epsilon$ is stored as `tol` (short for tolerance). Since each step is much shorter, SGD typically only checks for convergence at the end of each epoch.\n",
        "\n",
        "* To ensure that the function terminates, we should set a maximum limit for the number of gradient descent iterations irrespective of whether $\\theta$ converges or not. The limit is stored as `max_iter`. <font color=Red>The `max_iter` here refers to the max of epoch.</font>\n",
        "\n",
        "* `alpha` is the learning rate of the SGD algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.6. Predict [2 pts]**\n",
        "\n",
        "The `predict` function should predict the output for the data points in a given input data matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinearRegression:\n",
        "\n",
        "    \"\"\"\n",
        "    Linear Regression\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha: float, default=0.01\n",
        "        Learning rate\n",
        "    tol : float, default=0.0001\n",
        "        Tolerance for stopping criteria\n",
        "    max_iter : int, default=10000\n",
        "        Maximum number of iterations of gradient descent\n",
        "    theta_init: None (or) numpy.ndarray of shape (D + 1,)\n",
        "        The initial weights; if None, all weights will be zero by default\n",
        "    penalty : string, default = None\n",
        "        The type of regularization. The other acceptable options are l1 and l2\n",
        "    lambd : float, default = 1.0\n",
        "        The parameter regularization constant (i.e. lambda)\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    theta_ : numpy.ndarray of shape (D + 1,)\n",
        "        The value of the coefficients after gradient descent has converged\n",
        "        or the number of iterations hit the maximum limit\n",
        "    hist_theta_ : numpy.ndarray of shape (num_iter, D + 1) where num_iter is the number of gradient descent iterations\n",
        "        Stores theta_ after every gradient descent iteration\n",
        "    hist_cost_ : numpy.ndarray of shape (num_iter,) where num_iter is the number of gradient descent iterations\n",
        "        Stores cost after every gradient descent iteration\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, alpha = 0.01, tol=1e-4, max_iter = 100, theta_init = None, penalty = None, lambd = 0):\n",
        "        \n",
        "        # store meta-data\n",
        "        self.alpha = alpha\n",
        "        self.theta_init = theta_init\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.penalty = penalty\n",
        "        self.lambd = lambd\n",
        "\n",
        "        self.theta_ = None\n",
        "        self.hist_cost_ = None\n",
        "        self.hist_theta_ = None\n",
        "    \n",
        "    def compute_cost(self, theta, X, y):\n",
        "    \n",
        "        \"\"\"\n",
        "        Compute the cost/objective function.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta: numpy.ndarray of shape (D + 1,)\n",
        "            The coefficients\n",
        "        X: numpy.ndarray of shape (N, D + 1)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        cost: float\n",
        "            The cost as a scalar value\n",
        "        \"\"\"\n",
        "        \n",
        "        # TODO STARTS: Complete the function (should account for three cases - no penalty, l1 penalty, and l2 penalty)\n",
        "        \n",
        "        # TODO ENDS\n",
        "\n",
        "    def compute_gradient(self, theta, X, y):\n",
        "    \n",
        "        \"\"\"\n",
        "        Compute the gradient of the cost function.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta: numpy.ndarray of shape (D + 1,)\n",
        "            The coefficients\n",
        "        X: numpy.ndarray of shape (N, D + 1)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        gradient: numpy.ndarray of shape (D + 1,)\n",
        "            The gradient values\n",
        "        \"\"\"\n",
        "        \n",
        "        # TODO STARTS: Complete the function (should account for three cases - no penalty, l1 penalty, and l2 penalty)\n",
        "        \n",
        "        # TODO ENDS\n",
        "\n",
        "    def has_converged(self, theta_old, theta_new):\n",
        "\n",
        "        \"\"\"\n",
        "        Return whether gradient descent has converged.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta_old: numpy.ndarray of shape (D + 1,)\n",
        "            The weights prior to the update by gradient descent\n",
        "        theta_new: numpy.ndarray of shape (D + 1,)\n",
        "            The weights after the update by gradient descent\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        converged: bool\n",
        "            Whether gradient descent converged or not\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO START: Complete the function\n",
        "        \n",
        "        # TODO END\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the coefficients using gradient descent and store them as theta_.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Nothing\n",
        "        \"\"\"\n",
        "\n",
        "        N, D = X.shape\n",
        "\n",
        "        # Adding a column of ones at the beginning for the bias term\n",
        "        ones_col = np.ones((N, 1))\n",
        "        X = np.hstack((ones_col, X))\n",
        "        \n",
        "        # Initializing the weights\n",
        "        if self.theta_init is None:\n",
        "            theta_old = np.zeros((D + 1,))\n",
        "        else:\n",
        "            theta_old = self.theta_init\n",
        "\n",
        "        # Initializing the historical weights matrix\n",
        "        # Remember to append this matrix with the weights after every gradient descent iteration\n",
        "        self.hist_theta_ = np.array([theta_old])\n",
        "\n",
        "        # Computing the cost for the initial weights\n",
        "        cost = self.compute_cost(theta_old, X, y)\n",
        "\n",
        "        # Initializing the historical cost array\n",
        "        # Remember to append this array with the cost after every gradient descent iteration\n",
        "        self.hist_cost_ = np.array([cost])\n",
        "        \n",
        "        # TODO START: Complete the function\n",
        "        \n",
        "        # TODO END\n",
        "\n",
        "    def fit_sgd(self, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the coefficients using gradient descent and store them as theta_.\n",
        "        Make sure to save theta_ for each gradient descent conducted\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Nothing\n",
        "        \"\"\"\n",
        "\n",
        "        N, D = X.shape\n",
        "\n",
        "        # Adding a column of ones at the beginning for the bias term\n",
        "        ones_col = np.ones((N, 1))\n",
        "        X = np.hstack((ones_col, X))\n",
        "        \n",
        "        # Initializing the weights\n",
        "        if self.theta_init is None:\n",
        "            theta_old = np.zeros((D + 1,))\n",
        "        else:\n",
        "            theta_old = self.theta_init\n",
        "\n",
        "        # Initializing the historical weights matrix\n",
        "        # Remember to append this matrix with the weights after every gradient descent iteration\n",
        "        self.hist_theta_ = np.array([theta_old])\n",
        "\n",
        "        # Computing the cost for the initial weights\n",
        "        cost = self.compute_cost(theta_old, X, y)\n",
        "\n",
        "        # Initializing the historical cost array\n",
        "        # Remember to append this array with the cost after every gradient descent iteration\n",
        "        self.hist_cost_ = np.array([cost])\n",
        "        \n",
        "        # TODO START: Complete the function\n",
        "        \n",
        "        # TODO END\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        \"\"\"\n",
        "        Predict the target variable values for the data points in X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_hat: numpy.ndarray of shape (N,)\n",
        "            The predicted target variables values for the data points in X\n",
        "        \"\"\"\n",
        "\n",
        "        N = X.shape[0]\n",
        "        X = np.hstack((np.ones((N, 1)), X))\n",
        "        \n",
        "        # TODO START: Complete the function\n",
        "        \n",
        "        # TODO END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_lin_reg_compute_cost(StudentLinearRegression):\n",
        "    \n",
        "    test_case_theta = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression()\n",
        "    student_ans = student_lr_reg.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = 4.881828654157736\n",
        "    \n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(penalty=\"l1\", lambd=0.1)\n",
        "    student_ans = student_lr_reg.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = 4.94300429515773\n",
        "\n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(penalty=\"l2\", lambd=0.1)\n",
        "    student_ans = student_lr_reg.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = 4.919253244675344\n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "test_lin_reg_compute_cost(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_lin_reg_compute_gradient(StudentLinearRegression):\n",
        "    \n",
        "    test_case_theta = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression()\n",
        "    student_ans = student_lr_reg.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = [ 4.79663712, -3.53908485]\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(penalty=\"l1\", lambd=0.1)\n",
        "    student_ans = student_lr_reg.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = [ 4.79663712, -3.63908485]\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(penalty=\"l2\", lambd=0.1)\n",
        "    student_ans = student_lr_reg.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = [ 4.79663712, -3.66143613]\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "test_lin_reg_compute_gradient(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_lin_reg_has_converged(StudentLinearRegression):\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression()\n",
        "    test_case_theta_old = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_theta_new = np.array([1.624345, -0.611756])\n",
        "    student_ans = student_lr_reg.has_converged(test_case_theta_old, test_case_theta_new)\n",
        "    required_ans = True\n",
        "    \n",
        "    assert student_ans == required_ans\n",
        "\n",
        "test_lin_reg_has_converged(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_lin_reg_fit(StudentLinearRegression):\n",
        "    \n",
        "    student_lr_reg = StudentLinearRegression(max_iter=5)\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "    student_lr_reg.fit(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_reg.hist_theta_\n",
        "    required_ans = np.array([[ 0.        ,  0.        ,  0.        ],\n",
        "       [ 0.012     ,  0.00566085, -0.00773638],\n",
        "       [ 0.02351422,  0.01085581, -0.01491529],\n",
        "       [ 0.03457102,  0.01561393, -0.0215702 ],\n",
        "       [ 0.04519706,  0.01996249, -0.02773259],\n",
        "       [ 0.05541739,  0.02392713, -0.03343205]])\n",
        "    print(student_ans)\n",
        "    print(required_ans)\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "test_lin_reg_fit(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_lin_reg_fit_sgd(StudentLinearRegression):\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(max_iter=5)\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "    student_lr_reg.fit_sgd(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_reg.hist_theta_\n",
        "    required_ans = np.array([[ 0. ,         0. ,         0.        ],\n",
        "        [ 0.02 ,       0.03248691, -0.01223513],\n",
        "        [ 0.03968062,  0.02209216, -0.03335181],\n",
        "        [ 0.03696942,  0.01974587, -0.02711189],\n",
        "        [ 0.03512822,  0.01653332, -0.02571035],\n",
        "        [ 0.05419193,  0.02261539, -0.03046428],\n",
        "        [ 0.07200065,  0.05154291, -0.04135888],\n",
        "        [ 0.09021758,  0.04192125, -0.06090506],\n",
        "        [ 0.08488414,  0.03730565, -0.04862995],\n",
        "        [ 0.08114428,  0.0307803 , -0.04578314],\n",
        "        [ 0.09909665,  0.03650781, -0.05025993],\n",
        "        [ 0.11531376,  0.06284999, -0.06018085],\n",
        "        [ 0.13237995,  0.05383611, -0.07849234],\n",
        "        [ 0.12518748,  0.04761169, -0.0619386 ],\n",
        "        [ 0.1200793 ,  0.03869888, -0.05805022],\n",
        "        [ 0.13714127,  0.04414231, -0.06230497],\n",
        "        [ 0.15220209,  0.06860628, -0.07151852],\n",
        "        [ 0.16834802,  0.06007846, -0.0888426 ],\n",
        "        [ 0.15985172,  0.05272569, -0.06928804],\n",
        "        [ 0.15375991,  0.04209663, -0.06465091],\n",
        "        [ 0.17009366,  0.04730773, -0.06872406],\n",
        "        [ 0.18431405,  0.07040657, -0.07742348],\n",
        "        [ 0.19971005,  0.06227484, -0.0939429 ],\n",
        "        [ 0.19031372,  0.05414318, -0.07231689],\n",
        "        [ 0.18351709,  0.04228434, -0.06714324],\n",
        "        [ 0.19924207,  0.04730123, -0.07106459]])\n",
        "    print(student_ans)\n",
        "    print(required_ans)\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "test_lin_reg_fit_sgd(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_lin_reg_predict(StudentLinearRegression):\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(max_iter=5)\n",
        "    np.random.seed(1)\n",
        "    test_case_X = np.random.randn(50, 2)\n",
        "    test_case_y = np.random.randint(0, 2, 50)\n",
        "    student_lr_reg.fit(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_reg.predict(test_case_X)\n",
        "    print('student_ans', student_ans)\n",
        "    required_ans = np.array([0.04739416, 0.02735934, 0.02140787, 0.04634383, 0.04320043,\n",
        "       0.02836861, 0.03726417, 0.03808224, 0.03214353, 0.05166998,\n",
        "       0.05102933, 0.05639199, 0.0416892 , 0.03175554, 0.04895695,\n",
        "       0.03465034, 0.02912364, 0.03954521, 0.0396391 , 0.06440433,\n",
        "       0.03189335, 0.06016748, 0.03661307, 0.07146111, 0.05261461,\n",
        "       0.04180017, 0.03223834, 0.0500466 , 0.06128615, 0.05703506,\n",
        "       0.05467262, 0.04388664, 0.04648138, 0.07052753, 0.04140456,\n",
        "       0.02830984, 0.05608863, 0.0212115 , 0.05238969, 0.05514024,\n",
        "       0.04020117, 0.05048966, 0.04696158, 0.04438422, 0.05897309,\n",
        "       0.05443805, 0.03375689, 0.04794345, 0.04242038, 0.04869202])\n",
        "    \n",
        "    assert np.mean(np.abs(student_ans - required_ans)) <= 1e-2\n",
        "\n",
        "test_lin_reg_predict(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_lin_reg_predict_sgd(StudentLinearRegression):\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(max_iter=5)\n",
        "    np.random.seed(1)\n",
        "    test_case_X = np.random.randn(50, 2)\n",
        "    test_case_y = np.random.randint(0, 2, 50)\n",
        "    student_lr_reg.fit_sgd(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_reg.predict(test_case_X)\n",
        "    print('student_ans', student_ans)\n",
        "    required_ans = np.array([0.4113478,  0.28834918, 0.1227324,  0.39008601, 0.43987045, 0.17506316,\n",
        "                            0.40365951, 0.32180596, 0.32776898, 0.56721846, 0.63147595, 0.57385561,\n",
        "                            0.38334306, 0.31959516, 0.5517445,  0.39322627, 0.3213112,  0.45537132,\n",
        "                            0.48490982, 0.62956115, 0.32575875, 0.72747134, 0.37152396, 0.81428507,\n",
        "                            0.57451273, 0.42292006, 0.3905908,  0.56212164, 0.64126265, 0.62130162,\n",
        "                            0.65671342, 0.43645374, 0.47163355, 0.74245718, 0.29808437, 0.35882346,\n",
        "                            0.61700668, 0.15509352, 0.59866825, 0.60026664, 0.43537041, 0.5427557,\n",
        "                            0.49628385, 0.51805151, 0.65681787, 0.52965323, 0.36155917, 0.49471154,\n",
        "                            0.47184886, 0.57066729])\n",
        "    \n",
        "    assert np.mean(np.abs(student_ans - required_ans)) <= 1e-2\n",
        "\n",
        "test_lin_reg_predict_sgd(LinearRegression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **1.2. Synthetic dataset [Ungraded]**\n",
        "\n",
        "In this section we will first create some synthetic data on which we will run your linear regression implementation. We are creating 100 datapoints around the function y = mx + b, introducing Gaussian noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_samples = 100\n",
        "\n",
        "np.random.seed(1)\n",
        "noise = np.random.randn(num_samples, 1)\n",
        "X = np.random.randn(num_samples, 1)\n",
        "\n",
        "y_ideal = 11*X + 5\n",
        "y_real = (11*X + 5) + noise\n",
        "\n",
        "plt.plot(X, y_real, 'ro')\n",
        "plt.plot(X, y_ideal, 'b')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that this data is clearly regressable with a line, which, ideally, would be 11x + 5\n",
        "\n",
        "Train a linear regression model using gradient descent, you should see that training loss goes down with the number of iterations and obtain a theta that converges to a value very close to [b, m], which in this case, for 11x + 5, would be theta = [5, 11]\n",
        "\n",
        "Also, notice the effect of the type of regularization on the theta obtained (after convergence) as well as the testing MSE loss. Do they make sense, given what was discussed in class? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def test_synthetic_data_sgd(X, y, n_iter = 2000, penalty=None, lambd=0):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=37)\n",
        "  # Given that we want to get theta as the weights of the linear equation, we won't \n",
        "  # standardize in this section\n",
        "\n",
        "  alpha = 0.03  # Learning Rate\n",
        "\n",
        "  # # Train the model\n",
        "  lr_model = LinearRegression(alpha = alpha, tol=1e-4, max_iter = n_iter, penalty=penalty, lambd=lambd)\n",
        "  lr_model.fit(X_train,y_train[:, 0])\n",
        "  y_predict = lr_model.predict(X_test)\n",
        "  loss = sklearn.metrics.mean_squared_error(y_predict, y_test)\n",
        "  print()\n",
        "  print(\" Theta: {} \\n Norm of Theta: {} \\n Testing MSELoss: {}\".format(lr_model.theta_, np.linalg.norm(lr_model.theta_, ord=2), loss))\n",
        "\n",
        "  loss_history = lr_model.hist_cost_\n",
        "  plt.plot(range(len(loss_history)), loss_history)\n",
        "  plt.title(\"OLS Training Loss\")\n",
        "  plt.xlabel(\"iteration\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  if penalty == \"l1\":\n",
        "    plt.title(\"L1 Regularised Training Loss\")\n",
        "  elif penalty == \"l2\":\n",
        "    plt.title(\"L2 Regularised Training Loss\")\n",
        "  plt.show()\n",
        "\n",
        "test_synthetic_data_sgd(X, y_ideal, 500)\n",
        "test_synthetic_data_sgd(X, y_ideal, 500, \"l1\", 0.02)\n",
        "test_synthetic_data_sgd(X, y_ideal, 500, \"l2\", 0.02)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **1.3. Effect of polynomial degree on training and validation error [5 pts]**\n",
        "\n",
        "Now, we consider a dataset that was generated using some higher degree polynomial function of the input variable. We do not know the degree of the underlying polynomial. Let us assume it to be an unknown value \"p\" and try to estimate it.\n",
        "\n",
        "Polynomial regression hypothesis for one input variable  or feature (x) can be written as:\n",
        "> $y = w_0 + w_1x + w_2x^2 + ... + w_px^p $\n",
        "\n",
        "If you observe carefully, this can still be solved as a linear regression, where, instead of just 2 weights, we have p+1 weights, and the new features are higher order terms of the original feature. Using this idea, in this section, we will investigate how changing the assumed polynomial degree \"p\" in our model affects the training and validation error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "poly_reg_df = pd.read_csv('poly_reg.csv')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression as LinearRegressionSklearn\n",
        "\n",
        "def polynomial_regression(poly_reg_df, degrees):\n",
        "    \"\"\"\n",
        "    Runs polynomial regression on the dataset 'poly_reg_df' for all the powers in 'degrees'\n",
        "    \"\"\"\n",
        "\n",
        "    loss_train_list = []\n",
        "    loss_test_list = []\n",
        "\n",
        "    X_base = poly_reg_df.iloc[:, :-1].values\n",
        "    y = poly_reg_df.iloc[:, -1].values\n",
        "\n",
        "    for d in degrees:\n",
        "\n",
        "        # TODO START: Complete the function:\n",
        "        # 1. Transform the base feature X_base into its polynomial features of degree 'd' using PolynomialFeatures\n",
        "        # Set include_bias to be False\n",
        "        \n",
        "        \n",
        "        # 2. Preprocessing and splitting into train/test (70-30 ratio and random_state as 42)\n",
        "        \n",
        "        \n",
        "        # 3. Scale X_train and X_test appropriately \n",
        "        \n",
        "        \n",
        "        # 4. Use scikit-learn's LinearRegression (imported as LinearRegressionSklearn for you) to \n",
        "        # fit a linear model between the scaled version of X_train and y_train\n",
        "        \n",
        "        \n",
        "        # 5. Obtain predictions of the model on train and test data\n",
        "        \n",
        "\n",
        "        # 6. Compute the mean squared error and store it in loss_train and loss_test\n",
        "        \n",
        "\n",
        "        # 7. Append loss_train to loss_train_list and loss_test to loss_test_list\n",
        "        \n",
        "    return loss_train_list, loss_test_list\n",
        "    # TODO END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "degrees = np.arange(1, 9)\n",
        "\n",
        "loss_train_list, loss_test_list = polynomial_regression(poly_reg_df, degrees)\n",
        "\n",
        "# TODO START:\n",
        "# Plot the polynomial degrees (x-axis) against loss_train_list (y-axis) and loss_test_list (y-axis) in a single plot, with different colors.\n",
        "# Make sure to include x and y axis labels, legend as well as the title\n",
        "\n",
        "# TODO END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Attach the plot to your written homework solutions. Describe the trends in the plot obtained. Briefly explain the reasoning behind why this would happen.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **1.4. Effect of learning rate on gradient descent [5 pts]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"admit.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset contains two features - scores in two exams and the target variable is whether the student was admitted into a college or not. Your task for this question is to use this dataset and plot the variation of cost function with respect to the number of gradient descent iterations for different learning rates. Perform the following steps.\n",
        "\n",
        "1. Scale the features using StandardScaler\n",
        "2. For each of the learning rates - {0.001, 0.01, 0.03, 0.1, 1.0}, fit a <font color=Red>linear regression model</font> to the scaled data by running a maximum of 100 iterations of gradient descent with L2 penalty and $\\lambda$ as 0.001.\n",
        "3. Show the variation of the cost (stored in `hist_cost_`) with respect to the number of iterations for all the learning rates in the same plot.\n",
        "\n",
        "Submit the plot along with the written homework solutions. The plot should have an appropriate title, axes labels, and legend. Briefly comment on the effect of increasing learning rate and what would be the best learning rate among the four values based on the plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: The previously the plot showed a divergent behavior for alpha=0.25, but with linear regression, we do not find any learning rate that exhibits such behavior\n",
        "\n",
        "# STUDENT CODE STARTS:\n",
        "\n",
        "# STUDENT CODE ENDS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-m0iExC9bm4"
      },
      "source": [
        "# **2. Logistic Regression [23pts]** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9uBsNwS9c32"
      },
      "source": [
        "## **2.1. Logistic Regression Implementation [18 pts]**\n",
        "\n",
        "Implement logistic regression with both L1 and L2 regularization by completing the LogisticRegression class.  \n",
        "\n",
        "Your class must implement the following API:\n",
        "\n",
        "* `__init__(alpha, tol, max_iter, theta_init, penalty, lambd)`\n",
        "* `sigmoid(x)`\n",
        "* `compute_cost(theta, X, y)`\n",
        "* `compute_gradient(theta, X, y)`\n",
        "* `has_converged(theta_old, theta_new)`\n",
        "* `fit(X, y)`\n",
        "* `predict_proba(X)`\n",
        "* `predict(X)`\n",
        "\n",
        "Note that these methods have already been defined correctly for you in the LogisticRegression class. **DO NOT** change the API.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.1.1. Sigmoid Function [1 pt]**\n",
        "\n",
        "You should begin by implementing the `sigmoid` function.  As you may know, the sigmoid function $\\sigma(x)$ is mathematically defined as follows.\n",
        "\n",
        "> $\\sigma(x) = \\frac{1}{1\\ +\\ \\text{exp}(-x)}$\n",
        "\n",
        "**Be certain that your sigmoid function works with both vectors and matrices** --- for either a vector or a matrix, you function should perform the sigmoid function on every element.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.1.2. Cost Function [5 pts]**\n",
        "\n",
        "The `compute_cost` function should compute the cost for a given $\\theta$ vector. The cost is a scalar value given by:\n",
        "\n",
        "> $\n",
        "\\mathcal{L}({\\theta}) = -\\sum_{i =1}^N [ y_i\\log(h_{{\\theta}}({x}_i)) + (1 - y_i)\\log(1 - h_{{\\theta}}({x}_i))]\n",
        "$\n",
        "\n",
        "where\n",
        "> $\n",
        "h_{\\theta}(x_{i}) = \\sigma(\\theta^{T}x_{i})\n",
        "$\n",
        "\n",
        "\n",
        "L1 Regularisation Loss:\n",
        ">$\n",
        "\\mathcal{L1}({\\theta}) = \\mathcal{L}({\\theta}) + \\lambda \\sum_{j = 1}^D  |{\\theta}_j|\n",
        "$\n",
        "\n",
        "L2 Regularisation Loss:\n",
        ">$\n",
        "\\mathcal{L2}({\\theta}) = \\mathcal{L}({\\theta}) + \\lambda \\sum_{j = 1}^D  {\\theta}_j^2 \n",
        "$\n",
        "\n",
        "$N$ is the number of training samples and $D$ is the number of features (excluding the intercept term). $\\theta$ is a $D + 1$ dimensional vector, with the first element being the intercept term. Note that we do not include the intercept in the regularization terms.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.1.3. Gradient of the Cost Function [5 pts]**\n",
        "\n",
        "The `compute_gradient` function should compute the gradient of the cost function at a given $\\theta$.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.1.4. Convergence Check [1 pt]**\n",
        "\n",
        "The `has_converged` function should return whether gradient descent algorithm has converged or not. Refer 2.1.5 for convergence condition.\n",
        " \n",
        "---\n",
        "\n",
        "### **2.1.5. Training [3 pts]**\n",
        "\n",
        "The `fit` method should train the model via gradient descent, relying on the cost and gradient functions. The trained weights/coefficients must be stored as `theta_`. The weights start as a zero vector. The weights and the corresponding cost after every gradient descent iteration must be stored in `hist_theta_` and `hist_cost_` respectively.\n",
        "\n",
        "* The gradient descent stops or converges when $\\theta$ stops changing or changes negligibly between consecutive iterations, i.e., when \n",
        "$\\| {\\theta}_\\mathit{new} -  {\\theta}_\\mathit{old} \\|_2 \\leq \\epsilon$, \n",
        "for some small $\\epsilon$ (e.g., $\\epsilon$ = 1E-4). $\\epsilon$ is stored as `tol` (short for tolerance). \n",
        "\n",
        "* To ensure that the function terminates, we should set a maximum limit for the number of gradient descent iterations irrespective of whether $\\theta$ converges or not. The limit is stored as `max_iter`.\n",
        "\n",
        "* `alpha` is the learning rate of the gradient descent algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.1.6. Predict Probability [1 pt]**\n",
        "\n",
        "The `predict_probability` function should predict the probabilities that the data points in a given input data matrix belong to class 1.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.1.7. Predict [2 pts]**\n",
        "\n",
        "The `predict` function should predict the classes of the data points in a given input data matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVWOemIR8-09"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "\n",
        "    \"\"\"\n",
        "    Logistic Regression (aka logit, MaxEnt) classifier.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha: float, default=0.01\n",
        "        Learning rate\n",
        "    tol : float, default=0.0001\n",
        "        Tolerance for stopping criteria\n",
        "    max_iter : int, default=10000\n",
        "        Maximum number of iterations of gradient descent\n",
        "    theta_init: None (or) numpy.ndarray of shape (D + 1,)\n",
        "        The initial weights; if None, all weights will be zero by default\n",
        "    penalty : string, default = None\n",
        "        The type of regularization. The other acceptable options are l1 and l2\n",
        "    lambd : float, default = 1.0\n",
        "        The parameter regularisation constant (i.e. lambda)\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    theta_ : numpy.ndarray of shape (D + 1,)\n",
        "        The value of the coefficients after gradient descent has converged\n",
        "        or the number of iterations hit the maximum limit\n",
        "    hist_theta_ : numpy.ndarray of shape (num_iter, D + 1) where num_iter is the number of gradient descent iterations\n",
        "        Stores theta_ after every gradient descent iteration\n",
        "    hist_cost_ : numpy.ndarray of shape (num_iter,) where num_iter is the number of gradient descent iterations\n",
        "        Stores cost after every gradient descent iteration\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=0.01, tol=0.0001, max_iter=10000, theta_init=None, penalty = None, lambd = 1.0):\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "        self.theta_init = theta_init\n",
        "        self.penalty = penalty\n",
        "        self.lambd = lambd\n",
        "        self.theta_ = None\n",
        "        self.hist_cost_ = None\n",
        "        self.hist_theta_ = None\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        # a function needed for using cross_val_score function from sklearn.model_selection\n",
        "        return {\"alpha\": self.alpha, \"max_iter\": self.max_iter, \"lambd\" : self.lambd, \"penalty\" : self.penalty}\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the sigmoid value of the argument.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: numpy.ndarray\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        out: numpy.ndarray\n",
        "            The sigmoid value of x\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "\n",
        "        # TODO END\n",
        "\n",
        "    def compute_cost(self, theta, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the cost/objective function.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta: numpy.ndarray of shape (D + 1,)\n",
        "            The coefficients\n",
        "        X: numpy.ndarray of shape (N, D + 1)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        cost: float\n",
        "            The cost as a scalar value\n",
        "        \"\"\"\n",
        "        \n",
        "        # TODO START: Complete the function (should account for three cases - no penalty, l1 penalty, and l2 penalty)\n",
        "        # DO NOT use np.dot for this function as it can possibly return nan. Use a combination of np.nansum and np.multiply.\n",
        "        ...\n",
        "\n",
        "        # TODO END\n",
        "        \n",
        "    def compute_gradient(self, theta, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the gradient of the cost function.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta: numpy.ndarray of shape (D + 1,)\n",
        "            The coefficients\n",
        "        X: numpy.ndarray of shape (N, D + 1)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        gradient: numpy.ndarray of shape (D + 1,)\n",
        "            The gradient values\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO START: Complete the function (should account for three cases - no penalty, l1 penalty, and l2 penalty)\n",
        "        ...\n",
        "        # TODO END\n",
        "\n",
        "    def has_converged(self, theta_old, theta_new):\n",
        "\n",
        "        \"\"\"\n",
        "        Return whether gradient descent has converged.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta_old: numpy.ndarray of shape (D + 1,)\n",
        "            The weights prior to the update by gradient descent\n",
        "        theta_new: numpy.ndarray of shape (D + 1,)\n",
        "            The weights after the update by gradient descent\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        converged: bool\n",
        "            Whether gradient descent converged or not\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "        # TODO END\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the coefficients using gradient descent and store them as theta_.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Nothing\n",
        "        \"\"\"\n",
        "\n",
        "        N, D = X.shape\n",
        "\n",
        "        # Adding a column of ones at the beginning for the bias term\n",
        "        ones_col = np.ones((N, 1))\n",
        "        X = np.hstack((ones_col, X))\n",
        "        \n",
        "        # Initializing the weights\n",
        "        if self.theta_init is None:\n",
        "            theta_old = np.zeros((D + 1,))\n",
        "        else:\n",
        "            theta_old = self.theta_init\n",
        "\n",
        "        # Initializing the historical weights matrix\n",
        "        # Remember to append this matrix with the weights after every gradient descent iteration\n",
        "        self.hist_theta_ = np.array([theta_old])\n",
        "\n",
        "        # Computing the cost for the initial weights\n",
        "        cost = self.compute_cost(theta_old, X, y)\n",
        "\n",
        "        # Initializing the historical cost array\n",
        "        # Remember to append this array with the cost after every gradient descent iteration\n",
        "        self.hist_cost_ = np.array([cost])\n",
        "        \n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "\n",
        "        # TODO END\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "\n",
        "        \"\"\"\n",
        "        Predict the probabilities that the data points in X belong to class 1.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_hat: numpy.ndarray of shape (N,)\n",
        "            The predicted probabilities that the data points in X belong to class 1\n",
        "        \"\"\"\n",
        "\n",
        "        N = X.shape[0]\n",
        "        X = np.hstack((np.ones((N, 1)), X))\n",
        "        \n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "\n",
        "        # TODO END\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        \"\"\"\n",
        "        Predict the classes of the data points in X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred: numpy.ndarray of shape (N,)\n",
        "            The predicted class of the data points in X\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "        # TODO END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4edZSvLt8_Km"
      },
      "outputs": [],
      "source": [
        "def test_log_reg_sigmoid(StudentLogisticRegression):\n",
        "    \n",
        "    student_lr_clf = StudentLogisticRegression()\n",
        "    test_case = np.array([ 1.62434536, -0.61175641, -0.52817175, -1.07296862,  0.86540763])\n",
        "    student_ans = student_lr_clf.sigmoid(test_case)\n",
        "    required_ans = np.array([0.83539354, 0.35165864, 0.3709434 , 0.25483894, 0.70378922])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "test_log_reg_sigmoid(LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p37bE3an8_QK"
      },
      "outputs": [],
      "source": [
        "def test_log_reg_compute_cost(StudentLogisticRegression):\n",
        "    \n",
        "    test_case_theta = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression()\n",
        "    student_ans = student_lr_clf.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = 7.467975765663204\n",
        "\n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression(penalty=\"l1\", lambd=0.1)\n",
        "    student_ans = student_lr_clf.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = 7.52915138076548\n",
        "\n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression(penalty=\"l2\", lambd=0.1)\n",
        "    student_ans = student_lr_clf.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = 7.505400330283089\n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "test_log_reg_compute_cost(LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SI-neAfu8_Ub"
      },
      "outputs": [],
      "source": [
        "def test_log_reg_compute_gradient(StudentLogisticRegression):\n",
        "    \n",
        "    test_case_theta = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression()\n",
        "    student_ans = student_lr_clf.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = np.array([ 2.60573737, -2.20203139])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression(penalty=\"l1\", lambd=0.1)\n",
        "    student_ans = student_lr_clf.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = np.array([ 2.60573737, -2.30203139])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression(penalty=\"l2\", lambd=0.1)\n",
        "    student_ans = student_lr_clf.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = np.array([ 2.60573737, -2.32438267])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "test_log_reg_compute_gradient(LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhQ0YqCU-SeO"
      },
      "outputs": [],
      "source": [
        "def test_log_reg_has_converged(StudentLogisticRegression):\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression()\n",
        "    test_case_theta_old = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_theta_new = np.array([1.624345, -0.611756])\n",
        "    student_ans = student_lr_clf.has_converged(test_case_theta_old, test_case_theta_new)\n",
        "    required_ans = True\n",
        "\n",
        "    assert student_ans == required_ans\n",
        "\n",
        "test_log_reg_has_converged(LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJC7W7Lh-SjY"
      },
      "outputs": [],
      "source": [
        "def test_log_reg_fit(StudentLogisticRegression):\n",
        "    \n",
        "    student_lr_clf = StudentLogisticRegression(max_iter=5)\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "    student_lr_clf.fit(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_clf.hist_theta_\n",
        "    required_ans = np.array([[ 0.        ,  0.        ,  0.        ],\n",
        "                             [ 0.005     , -0.00597503,  0.00564325],\n",
        "                             [ 0.01006813, -0.01184464,  0.0111865 ],\n",
        "                             [ 0.01520121, -0.01761226,  0.01663348],\n",
        "                             [ 0.02039621, -0.02328121,  0.02198778],\n",
        "                             [ 0.02565018, -0.0288547 ,  0.02725288]])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "test_log_reg_fit(LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toqYtIpX-Snf"
      },
      "outputs": [],
      "source": [
        "def test_log_reg_predict_proba(StudentLogisticRegression):\n",
        "    \n",
        "    student_lr_clf = StudentLogisticRegression(max_iter=5)\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "    student_lr_clf.fit(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_clf.predict_proba(test_case_X)\n",
        "    required_ans = np.array([0.49052814, 0.5029122 , 0.48449386, 0.48864172, 0.50241207])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "test_log_reg_predict_proba(LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6yb0tUR-alL"
      },
      "outputs": [],
      "source": [
        "def test_log_reg_predict(StudentLogisticRegression):\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression(max_iter=5)\n",
        "    np.random.seed(1)\n",
        "    test_case_X = np.random.randn(50, 2)\n",
        "    test_case_y = np.random.randint(0, 2, 50)\n",
        "    student_lr_clf.fit(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_clf.predict(test_case_X)\n",
        "    required_ans = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
        "                             0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1])\n",
        "\n",
        "    assert np.mean(np.abs(student_ans - required_ans)) <= 0.02\n",
        "\n",
        "test_log_reg_predict(LogisticRegression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R1hHz4C-hWy"
      },
      "source": [
        "## **2.2. Effect of learning rate on gradient descent [5 pts]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGik8OTr-rKn"
      },
      "source": [
        "Run the below cell to download the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekqfVecV-d1b"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"admit.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnrAuubQ-v10"
      },
      "source": [
        "The dataset contains two features - scores in two exams and the target variable is whether the student was admitted into a college or not. Your task for this question is to use this dataset and plot the variation of cost function with respect to the number of gradient descent iterations for different learning rates. Perform the following steps.\n",
        "\n",
        "1. Scale the features using StandardScaler\n",
        "2. For each of the learning rates - {0.001, 0.01, 0.1, 0.25}, fit a <font color=Red>logistic regression model</font> to the scaled data by running a maximum of 100 iterations of gradient descent with L2 penalty and $\\lambda$ as 0.001.\n",
        "3. Show the variation of the cost (stored in `hist_cost_`) with respect to the number of iterations for all the learning rates in the same plot.\n",
        "\n",
        "Submit the plot along with the written homework solutions. The plot should have an appropriate title, axes labels, and legend. Briefly comment on the effect of increasing learning rate and what would be the best learning rate among the four values based on the plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "tpRpmIHd-d3D",
        "outputId": "d79decb7-6789-4aa9-8cf1-cb28aa7e230a"
      },
      "outputs": [],
      "source": [
        "# STUDENT CODE STARTS:\n",
        "...\n",
        "\n",
        "# STUDENT CODE ENDS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWRRLni5Pz_Q"
      },
      "source": [
        "# **3. K-Nearest Neighbors [10 pts]**\n",
        "While doing classification, KNN searches the memorized training instances for the K instances that most closely resemble the new instance and assigns to it the most common class. An alternate way of understanding KNN is by looking at the learned decision boundaries. In this problem, you will implement a function to classify points in the X-Y coordinates using [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). The training dataset used is the [Iris Dataset](https://archive.ics.uci.edu/ml/datasets/iris), and each point in the 2d-space will be classified into one of the three classes using its x-coordinate(sepal length) and y-coordinate(sepal width)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mze8zKRY4c3z"
      },
      "source": [
        "## **3.1. Load Iris Dataset**\n",
        "Please complete the load_dataset function to\n",
        "- Populate X_train with iris dataset features. We use only the sepal length and width for this exercise, i.e. the first two columns in the dataset\n",
        "- Populate y_train with labels (species)\n",
        "- return X_train and y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay4oEx1x4eeu"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "def load_iris_dataset():\n",
        "    '''\n",
        "    Args:\n",
        "        None\n",
        "    Returns: X_train, y_train\n",
        "    Notes:\n",
        "        1. Please do not change the provided code\n",
        "    '''\n",
        "    # import training data\n",
        "    iris = datasets.load_iris()\n",
        "\n",
        "    # TODO:\n",
        "    # Examine the iris variable and initialize the following variables appropriately:\n",
        "    # 1. X_train - Shape (m, 2): Only use the sepal length and width\n",
        "    # 2. y_train - Shape (m, ): target labels \n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    #### Student code ends ####\n",
        "\n",
        "# Load the iris dataset first\n",
        "X_train, y_train = load_iris_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYAL32DU2cWJ"
      },
      "source": [
        "## **3.2. Standardise the Features [4 pts]**\n",
        "Please complete the standardise_features function to \n",
        "standardize the features by subtracting the mean and scaling to unit variance. i.e \n",
        "\n",
        " z = (x - u) / s\n",
        "\n",
        "where u is the mean of the training data and s is the standard deviation of the training data.\n",
        "\n",
        "Here, centering and scaling need to happen independently on each feature (column) of the training data.\n",
        "\n",
        "**Note**: \n",
        "\n",
        "Please implement this function yourself. \n",
        "\n",
        "**Do NOT use sklearn's StandardScaler**. \n",
        "\n",
        "You are encouraged to use numpy as well as numpy vectorisation/broadcasting techniques to speed up the calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yn2eDuOj170T"
      },
      "outputs": [],
      "source": [
        "def standardise_features(X_train):\n",
        "\n",
        "  '''\n",
        "  Args:\n",
        "      X_train: Training dataset\n",
        "  Returns: X_train (After Standardization)\n",
        "  Notes:\n",
        "      1. Please do not change the provided code\n",
        "  '''\n",
        "\n",
        "  # TODO:\n",
        "  # 1. Calculate columnwise means and standard deviations\n",
        "  # 2. Perform columnwise standardisation i.e. subtract off mean and divide by standard deviation\n",
        "  # 3. Return the standardised data\n",
        "  #### Student code starts ####\n",
        "  ...\n",
        "  \n",
        "  #### Student code ends ####\n",
        "\n",
        "X_train = standardise_features(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHobVEPHxXa8"
      },
      "source": [
        "## **3.3. Plot KNN Decision Boundary [6 pts]**\n",
        "Please complete the plot_KNN_boundary function to\n",
        "- train a KNN classifier with k neighbors using the provided X_train and y_train\n",
        "- make predictions using X_test and save the result as 'y_test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQNVfjcz4tj5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn import datasets\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def plot_KNN_boundary(k, X_train, y_train):\n",
        "    '''\n",
        "    Args:\n",
        "        k: Number of neighbors to use for kneighbors queries.\n",
        "        X_train: Training dataset\n",
        "        y_train: Labels\n",
        "\n",
        "    Returns:\n",
        "    Notes:\n",
        "        1. Please do not change the provided code\n",
        "        2. save the predicted labels as y_test for plotting\n",
        "    '''\n",
        "\n",
        "    # Mesh 2d space into grid to generate X_test and y_test\n",
        "    h = 0.02\n",
        "    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
        "    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                            np.arange(y_min, y_max, h))\n",
        "    X_test = np.c_[xx.ravel(), yy.ravel()]\n",
        "    y_test = np.zeros(xx.shape)\n",
        "\n",
        "    # TODO:\n",
        "    # 1. train a KNN classifier\n",
        "    # 2. save the predictions on X_test in y_test\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code ends ####\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    y_test = y_test.reshape(xx.shape)\n",
        "    cmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\n",
        "    cmap_bold = ['darkorange', 'c', 'darkblue']\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, y_test, cmap=cmap_light)\n",
        "\n",
        "    # Also plot the training points\n",
        "    iris_target_names = ['setosa', 'versicolor', 'virginica']\n",
        "    sns.scatterplot(x=X_train[:, 0], y=X_train[:, 1], hue=map(lambda y: iris_target_names[y], y_train),\n",
        "                    palette=cmap_bold, alpha=1.0, edgecolor=\"black\")\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "    plt.title(\"3-Class classification (k = %i)\" % (k))\n",
        "    plt.xlabel(\"standardised sepal length\")\n",
        "    plt.ylabel(\"standardised sepal width\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTvb0XeDyAEC"
      },
      "source": [
        "Explore the effect of changing k on the learned decision boundaries. \n",
        "- Submit the plots along with the written homework solutions. \n",
        "- State whether the model underfits/overfits as k increases and explain why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wbUJFxTcqTgP",
        "outputId": "12e981a7-d023-4159-cee1-43e5cbfd49a1"
      },
      "outputs": [],
      "source": [
        "# Plot KNN decision boundaries with different k values\n",
        "def visualize_KNN(X_train, y_train):\n",
        "\n",
        "    k_list = [1, 4, 11, 15]\n",
        "    \n",
        "    # TODO:\n",
        "    # 1. Call plot_KNN_boundary function for each value of k in k_list\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code ends ####\n",
        "\n",
        "visualize_KNN(X_train, y_train) ### Comment out this line when submitting ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2Cm728599Wt"
      },
      "source": [
        "# **4. Measures of Impurity and their Reduction [15 pts]**\n",
        "To grow a classification tree, instead of a binary error (1/0), measures of impurity are used to see how good a leaf node is. Recall that we discussed about entropy being one such measure of impurity. We will be working with entropy and comparing it to another metric called the gini index. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FHSKW6f_7yg"
      },
      "source": [
        "## **4.1. Measures of Impurity [9 pts]**\n",
        "\n",
        "For this problem, consider that you have a binary classification problem of two classes, the positive class $1$ and the negative class $0$. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aELrA7A_WbJ8"
      },
      "source": [
        "### **4.1.1. Entropy [2 pts]**\n",
        "\n",
        "Please complete the entropy function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dyEposN3Sph"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def cross_entropy(prob_class1):  \n",
        "\n",
        "    \"\"\"\n",
        "    Returns the cross-entropy value of a node given the probability of a sample belonging to class 1 in the node.\n",
        "\n",
        "    Args: \n",
        "        prob_class1: The probability of a sample belonging to class 1 in a decision tree node\n",
        "      \n",
        "    Returns:\n",
        "        ce: The cross-entropy value for the node\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code ends #### \n",
        "    \n",
        "assert cross_entropy(0.5) == 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxp71lMkdoVP"
      },
      "source": [
        "### **4.1.2. Gini Index [2 pts]**\n",
        "\n",
        "Gini index is another measure of impurity. For an K-class classification problem, gini index is calculated as follows.\n",
        "\n",
        "$$\\text{Gini Index} = \\sum_{k = 1}^{K} p_k(1 - p_k)$$\n",
        "\n",
        "Complete the following function for calculating the gini index of a binary-class problem (k = 2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj__P_XV3Uj5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def gini_index(prob_class1):  \n",
        "\n",
        "    \"\"\"\n",
        "    Returns the gini-index value of a node given the probability of a sample belonging to class 1 in the node.\n",
        "\n",
        "    Args: \n",
        "        prob_class1: The probability of a sample belonging to class 1 in a decision tree node\n",
        "      \n",
        "    Returns:\n",
        "        gi: The gini-index value for the node\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code ends #### \n",
        "\n",
        "assert gini_index(0.5) == 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "837Ex93tXP1c"
      },
      "source": [
        "### **4.1.2. Plot [5 pts]**\n",
        "\n",
        "Please complete the impurity_measures_plot function and generate a plot of the entropy and gini index values with respect to the class 1 probability values. Both the impurity measures should be on the same plot.\n",
        "\n",
        "- Submit the generated plot along with the written homework solutions.\n",
        "- Make sure the plot has a title, legend and axes labels.\n",
        "- Comment on why cross entropy and gini index are suitable measures of impurity based on the plot. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "J1lZ_hdi-Bwc",
        "outputId": "2f575e4b-969a-4d1d-f749-412f17029646"
      },
      "outputs": [],
      "source": [
        "def impurity_measures_plot():\n",
        "\n",
        "    '''\n",
        "    Plots the cross entropy and gini index values with respect to the probability values of class 1.\n",
        "\n",
        "    Args: \n",
        "\n",
        "    Returns:\n",
        "\n",
        "    Notes:\n",
        "        1. Please do not change the provided code\n",
        "        2. Both cross entropy and gini index should be on the same scatter plot\n",
        "    '''\n",
        "\n",
        "    prob_class1_arr = np.arange(1, 1000)/1000\n",
        "    ce_arr = np.array([cross_entropy(p) for p in prob_class1_arr])\n",
        "    gi_arr = np.array([gini_index(p) for p in prob_class1_arr])\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code ends ####\n",
        "\n",
        "impurity_measures_plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow8qK0lZ2_JM"
      },
      "source": [
        "## **4.2. Reduction in Impurity [6 pts]**\n",
        "\n",
        "Recall that we also discussed information gain which is the change in entropy from the parent node to the children nodes. Gini reduction is similar to information gain except you replace entropy values with gini index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMeFW6DgPCFe"
      },
      "source": [
        "### **4.2.1. Information Gain [3 pts]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZvwxSjc3dHT"
      },
      "outputs": [],
      "source": [
        "def information_gain(num_samples_parent, num_class1_parent, num_samples_child1, num_class1_child1):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Args: \n",
        "        num_samples_parent: Number of samples in the parent node\n",
        "        num_class1_parent: Number of samples of class 1 in parent node\n",
        "        num_samples_child1: Number of samples in the first child node\n",
        "        num_class1_child1: Number of samples of class 1 in the first child node\n",
        "\n",
        "    Returns:\n",
        "        ig: Information Gain\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    # 1. You will need to calculate cross-entropy for the parent and child nodes\n",
        "    # 2. Use the above entropies to finally calculate information gain\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code ends ####\n",
        "\n",
        "\n",
        "assert np.abs(information_gain(100, 60, 30, 5) - 0.251) < 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GUxtCnOPTBQ"
      },
      "source": [
        "### **4.2.2. Gini Reduction [3 pts]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCiXGB--6MR5"
      },
      "outputs": [],
      "source": [
        "def gini_reduction(num_samples_parent, num_class1_parent, num_samples_child1, num_class1_child1):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Args: \n",
        "        num_samples_parent: Number of samples in the parent node\n",
        "        num_class1_parent: Number of samples of class 1 in parent node\n",
        "        num_samples_child1: Number of samples in the first child node\n",
        "        num_class1_child1: Number of samples of class 1 in the first child node\n",
        "\n",
        "    Returns:\n",
        "        gr: Gini Reduction\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code ends ####\n",
        "\n",
        "assert np.abs(gini_reduction(100, 60, 30, 5) - 0.161) < 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JG4XjHNPuC2"
      },
      "source": [
        "# **5. Decision Tree [29 pts]**\n",
        "\n",
        "In this section you will be training a decision tree classifier to predict the presence of diabetes in a person given various input features. The diabetes dataset that we are using is from the [2013-2014  National Health and Nutrition Examination Survey (NHANES)](https://wwwn.cdc.gov/nchs/nhanes/Default.aspx). We have reduced the dataset to only 20 features but the original dataset had over 1,800 features. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC_OXMz-oRKD"
      },
      "source": [
        "## **5.1. Load Datasets**\n",
        "\n",
        "Read the files `diabetes_train_data.csv` and `diabetes_test_label.csv` into train_df and test_df in the `load_diabetes_datasets` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQ4pBCjm_5-K"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_diabetes_datasets():\n",
        "    '''\n",
        "    Args:\n",
        "        None\n",
        "    Returns: \n",
        "        train_df, test_df\n",
        "    '''\n",
        "    \n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code ends ####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_06na9yloolX"
      },
      "source": [
        "## **5.2. Preprocess Datasets [10 pts]**\n",
        "\n",
        "The datasets we have provided are not ready-to-use for machine learning and requires preprocessing. We want you to perform feature selection and handle missing values in both the training and test datasets. \n",
        "\n",
        "### **5.2.1. Feature Selection**\n",
        "\n",
        "For feature selection, you should retain the following features at least and experiment including/excluding the remaining features. \n",
        "\n",
        "- 'RIDAGEYR'\n",
        "- 'BMXWAIST'\n",
        "- 'BMXHT'\n",
        "- 'LBXTC'\n",
        "- 'BMXLEG'\n",
        "- 'BMXWT'\n",
        "- 'BMXBMI'\n",
        "- 'RIDRETH1'\n",
        "- 'BPQ020'\n",
        "- 'ALQ120Q'\n",
        "- 'DMDEDUC2'\n",
        "- 'RIAGENDR'\n",
        "- 'INDFMPIR'\n",
        "\n",
        "The column `DIABETIC` in the training dataset is the target variable. \n",
        "\n",
        "### **5.2.2. Handling Missing Values**\n",
        "\n",
        "We recommend you to drop rows with missing values in the training set. However, you should not drop rows with missing values in the test set. Instead, you should impute missing values in the test set with the mean of the corresponding columns in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoEdsqDgBC49"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = load_diabetes_datasets()\n",
        "\n",
        "# Preprocessing\n",
        "def preprocess_datasets(train_df, test_df):\n",
        "    '''\n",
        "    Args:\n",
        "        train_df\n",
        "        test_df\n",
        "    Returns:\n",
        "        train_df (preprocessed)\n",
        "        test_df (preprocessed)\n",
        "    Note:\n",
        "        1. At least the following columns should be present in the final train_df:\n",
        "            - 'RIDAGEYR'\n",
        "            - 'BMXWAIST'\n",
        "            - 'BMXHT'\n",
        "            - 'LBXTC'\n",
        "            - 'BMXLEG'\n",
        "            - 'BMXWT'\n",
        "            - 'BMXBMI'\n",
        "            - 'RIDRETH1'\n",
        "            - 'BPQ020'\n",
        "            - 'ALQ120Q'\n",
        "            - 'DMDEDUC2'\n",
        "            - 'RIAGENDR'\n",
        "            - 'INDFMPIR'\n",
        "            - 'DIABETIC'\n",
        "        2. test_df will have all the columns in train_df except the 'DIABETIC' column \n",
        "        3. Drop any rows in train_df that have missing values\n",
        "        4. DO NOT drop rows with missing values test_df. Impute missing values in test_df with the means of the corresponding columns in train_df. \n",
        "    '''\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code ends ####\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0k07SoBrrD6"
      },
      "source": [
        "## **5.3. Decision Tree Training with Pruning [14 pts]**\n",
        "\n",
        "Next, we will be fitting a decision tree classifier and prune the tree appropriately. The `DecisionTreeClassifier` in scikit-learn uses a way of pruning called **Minimal Cost-Complexity Pruning**. We won't cover the specifics, but you can learn more from this [link](https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning) if you wish. But, you don't need to learn the details in order to use it effectively. The amount of pruning is entirely dependent on the value of the `ccp_alpha` parameter. In order to tune the `ccp_alpha` parameter, you will use [cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html). The purpose of cross-validation is to estimate how well a model will generalize on unseen data.\n",
        "\n",
        "Implement the function `best_ccp_alpha_f1` to do automatic tuning of the `ccp_alpha` parameter.  Your function should vary the value of the `ccp_alpha` parameter and return the value for `ccp_alpha` with the highest cross-validation F1 score over the given dataset `train_df`. The sklearn library has a [built-in function](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.cost_complexity_pruning_path) to generate a list of effective ccp_alphas. Given the imbalanced nature of the dataset, most of the people in the data set are non-diabetic. You can get a model with very high test accuracy by always predicting no one is diabetic. To address this problem, more importance should be given to the [F1 score](https://en.wikipedia.org/wiki/F-score) of your model rather than the classification accuracy.\n",
        "\n",
        "For this problem, you need to have at least 80% accuracy and a F1 score of 0.2 on the test dataset to get full points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOlr7qzjxIxS"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "train_df, test_df = preprocess_datasets(train_df, test_df)\n",
        "\n",
        "def best_ccp_alpha_f1(train_df):\n",
        "    \"\"\"\n",
        "    Returns the pruning parameter (best_ccp_alpha) with the highest cross-validation F1 score along with the \n",
        "    five cross-validation F1 scores corresponding (cv_f1_scores).\n",
        "\n",
        "    Args:\n",
        "        train_df\n",
        "\n",
        "    Returns:\n",
        "        best_ccp_alpha: the tuned best ccp alpha value\n",
        "        cv_f1_scores: the five cross-validation F1 scores\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "\n",
        "    #### Student code ends ####    \n",
        "\n",
        "def refit_and_predict(train_df, test_df, best_ccp_alpha):\n",
        "    \"\"\"\n",
        "    Fit a decision tree classifier on the training data using the best_ccp_alpha value and output the predictions on the\n",
        "    test set.\n",
        "\n",
        "    Args:\n",
        "        train_df\n",
        "        test_df\n",
        "        best_ccp_alpha\n",
        "\n",
        "    Returns:\n",
        "        y_test_pred: The predicted values for the test set\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code ends ####\n",
        "\n",
        "best_ccp_alpha, cv_f1_scores = best_ccp_alpha_f1(train_df)\n",
        "y_test_pred = refit_and_predict(train_df, test_df, best_ccp_alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cal_accuracy_diabetes(y_test_pred):\n",
        "    test_df = pd.read_csv(\"diabetes_test_label.csv\")\n",
        "    test_y = test_df['DIABETIC']\n",
        "    accuracy = np.mean(y_test_pred == test_y)\n",
        "    print(\"accuracy: {}\".format(accuracy))\n",
        "    return accuracy\n",
        "\n",
        "def cal_f1_score_diabetes(y_test_pred):\n",
        "    test_df = pd.read_csv(\"diabetes_test_label.csv\")\n",
        "    test_y = test_df['DIABETIC']\n",
        "    tp = np.sum((y_test_pred == 1) & (test_y == 1))\n",
        "    fp = np.sum((y_test_pred == 1) & (test_y == 0))\n",
        "    fn = np.sum((y_test_pred == 0) & (test_y == 1))\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f1_score = 2 * precision * recall / (precision + recall)\n",
        "    print(\"f1_score: {}\".format(f1_score))\n",
        "    return f1_score\n",
        "\n",
        "acc = cal_accuracy_diabetes(y_test_pred)\n",
        "f1 = cal_f1_score_diabetes(y_test_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khcu-0rBipeR"
      },
      "source": [
        "## **5.4. Computing Confidence Intervals [5 pts]**\n",
        "\n",
        "Even though you may have computed the average F1 score across the held-out folds during cross validation, how confident can you be that the number you computed is the true F1 score for that set of features? If you try rerunning your code with a different random seed, you may actually get a different F1 score. But which one is right?\n",
        "\n",
        "In order to answer this question, we will compute a confidence interval based on the Student's t-distribution, which will tell us with 99\\% confidence that the true mean is within a lower and upper bound. To compute the confidence interval, we need to compute the sample mean, $\\bar{x}$, sample standard deviation, $S$, and the number of observations for each classifier, $n$. ***In our specific case, the number of observations should be 5 because we have 5 reported F1 scores from cross-validation.***\n",
        "\n",
        "Then, the confidence interval is computed by\n",
        "    \n",
        "$$\\bar{x} \\pm t \\cdot \\frac{S}{\\sqrt{n}}$$\n",
        "\n",
        "Here, $t$ is the critical value, which we can look up using the provided t-table (https://www.stat.colostate.edu/inmem/gumina/st201/pdf/Utts-Heckard_t-Table.pdf). (Round up the critical value to the second digit below the decimal point) For example, when $n=10$, if we are looking for a 99\\% confidence interval, then the number in the 99\\% confidence column with degrees of freedom of $n-1=9$ would be $t=3.25$. Then, we can plug in all of the statistics into the confidence interval formula and get a range of values for which we are 99\\% confident that the true F1 score of the classifier falls between.\n",
        "\n",
        "For this computation, we should use the unbiased estimator of the variance, which means that the degrees of freedom on the standard deviation calculation must be set. Look in the optional arguments of np.std to learn more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mDdt6GfddPn"
      },
      "outputs": [],
      "source": [
        "def calculate_confidence_interval(cv_f1_scores):\n",
        "    '''\n",
        "    Args:\n",
        "        cv_f1_scores      :   np.array, reported cross-validation F1 scores\n",
        "    Returns:\n",
        "        interval    :   np.array, lower bound and upper bound of the 99% confidence interval\n",
        "    '''\n",
        "    \n",
        "    # TODO: this function should be able to handle two confidence levels of 99% and 80%\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code starts ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7oV-VY03B6S"
      },
      "outputs": [],
      "source": [
        "def test_confidence_intervals():\n",
        "    data = np.array([15.6, 16.2, 22.5, 20.5, 16.4])\n",
        "    result = np.round(calculate_confidence_interval(data), 3)\n",
        "    interval = np.array([11.918, 24.562])\n",
        "    assert (np.array_equal(interval, result))\n",
        "\n",
        "test_confidence_intervals()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovIZMkN131ok"
      },
      "source": [
        "# **6. Logistic Regression VS. Decision Tree [18pts]**\n",
        "\n",
        "Let's revisit the Logistic Regression model and compare the performance of it and with Decision Tree on the Diabietes dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEw4D5tpbXR2"
      },
      "source": [
        "## **6.1. Fit the Logistic Regression on Diabetes dataset [5pts]**\n",
        "\n",
        "Fit a simple logistic regression on the training data using l2 penalty, $\\alpha$ = 0.01, maximum of iterations = 1000, and weight for the regularization consant for the l2 penalty  term is 0.001. \n",
        "You should be rescaling features using MinMaxScaler from sklearn.preprocessing to make sure that the features are properly scaled for learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDAFJTHJbTEv"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def fit_and_predict_logistic(train_df, test_df):\n",
        "    \"\"\"\n",
        "    Fit a logistic regression classifier on the training data and output the predictions on the\n",
        "    test set.\n",
        "\n",
        "    Args:\n",
        "        train_df\n",
        "        test_df\n",
        "\n",
        "    Returns:\n",
        "        y_test_pred: The predicted values for the test set\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code ends ####\n",
        "\n",
        "y_test_pred_logistic = fit_and_predict_logistic(train_df, test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er95wB7MtDnq"
      },
      "source": [
        "## **6.2. Fit the Logistic Regression on Diabetes dataset [13pts]**\n",
        "\n",
        "Find the best alpha that has the highest mean value of f1 scores across 5-fold cross-validation. You should use the same min-max scaler and your candidates for `alpha` should be `[0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05]`. Use the same values for `max_iter`, `lambd` and `penalty` as 5.1. You can still use the `cross_val_score` from scikitlearn here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pThjfGFHnVJC",
        "outputId": "9689f002-9110-481d-ab3b-4e1845333b97"
      },
      "outputs": [],
      "source": [
        "import warnings, tqdm\n",
        "\n",
        "#suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def best_alpha_f1_logistic(train_df):\n",
        "    \"\"\"\n",
        "    Returns the learning rate (alpha) with the highest mean cross-validation F1 score along with the \n",
        "    five cross-validation F1 scores corresponding to the alpha(cv_f1_scores).\n",
        "\n",
        "    Args:\n",
        "        train_df\n",
        "\n",
        "    Returns:\n",
        "        best_alpha: the tuned best ccp alpha value\n",
        "        cv_f1_scores: the five cross-validation F1 scores for the best_alpha\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "\n",
        "    #### Student code ends ####    \n",
        "\n",
        "def refit_and_predict_logistic(train_df, test_df, best_alpha):\n",
        "    \"\"\"\n",
        "    Fit a logistic regressor on the training data using the best_alpha value and output the predictions on the\n",
        "    test set.\n",
        "\n",
        "    Args:\n",
        "        train_df\n",
        "        test_df\n",
        "        best_alpha\n",
        "\n",
        "    Returns:\n",
        "        y_test_pred: The predicted values for the test set\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    \n",
        "    ...\n",
        "    #### Student code ends ####\n",
        "\n",
        "best_alpha, cv_f1_scores = best_alpha_f1_logistic(train_df)\n",
        "y_test_pred_refit_logistic = refit_and_predict_logistic(train_df, test_df, best_alpha)\n",
        "confidence_interval_logistic = calculate_confidence_interval(cv_f1_scores)\n",
        "print(best_alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iAUrum0tJSJ"
      },
      "source": [
        "End of trials, should not be included"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **7. [25 pts] AdaBoost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "def prepare_final_cleaned_df(df):\n",
        "  df = df.drop([\"Unnamed: 0\"], axis=1)\n",
        "  df[\"mdct\"] = pd.to_datetime(df[\"mdct\"])\n",
        "  df.loc[df[\"gust\"].isna(),\"gust\"] = 0\n",
        "  df.loc[df[\"gbrd\"].isna(),\"gbrd\"] = 0\n",
        "  df.loc[df[\"wdsp\"].isna(),\"wdsp\"] = 0\n",
        "  df.loc[df[\"dewp\"].isna(),\"dewp\"] = 0\n",
        "  df.loc[df[\"dmin\"].isna(),\"dmin\"] = 0\n",
        "  df.loc[df[\"dmax\"].isna(),\"dmax\"] = 0\n",
        "  df = df[df[\"temp\"] != 0]\n",
        "  df = df.drop(columns=[\"prcp\"])\n",
        "\n",
        "  left_df = df.copy()\n",
        "  right_df = df.copy()\n",
        "  right_df[\"mdct\"] = right_df[\"mdct\"].apply(lambda x : x + datetime.timedelta(hours=1))\n",
        "\n",
        "  columns = [\"stp\", \"smax\", \"smin\", \"gbrd\", \"dewp\", \"tmax\", \n",
        "            \"dmax\", \"tmin\", \"dmin\", \"hmdy\", \"hmax\",\n",
        "            \"hmin\", \"wdsp\", \"wdct\", \"gust\", \"temp\"]\n",
        "          \n",
        "  merged_df = pd.merge(left_df, right_df, \"left\", on=[\"wsid\",\"mdct\"], indicator=True)\n",
        "  merged_df = merged_df[merged_df['_merge'] == \"both\"]\n",
        "\n",
        "  columns_x = [x + \"_x\" for x in columns]\n",
        "  columns_y = [x + \"_y\" for x in columns]\n",
        "  \n",
        "  merged_df[columns] = merged_df[columns_x].values - merged_df[columns_y].values\n",
        "  merged_df = merged_df.drop(columns=columns_x)\n",
        "  merged_df = merged_df.drop(columns=columns_y)\n",
        "  merged_df = merged_df.drop(columns=[\"_merge\", \"mdct\", \"wsid\"])\n",
        "\n",
        "  final_cleaned_df = merged_df.copy()\n",
        "\n",
        "  final_cleaned_df.loc[final_cleaned_df[\"temp\"] >= 0, \"temp\" ] = 1\n",
        "  final_cleaned_df.loc[final_cleaned_df[\"temp\"] < 0, \"temp\" ] = 0\n",
        "  return final_cleaned_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **7.1.  [3 pts] Logistic regression with sample weights**\n",
        "\n",
        "As you will have learnt from the lectures, AdaBoost fits weak learners (here, logistic regression model)  in each iteration, to a dataset with weights $w_i$ attached to each sample $(x_i, y_i)$. The loss function now becomes:\n",
        "\n",
        "> $\n",
        "\\mathcal{L}({\\theta}) = -\\sum_{i =1}^N w_{i} \\times [ y_i\\log(h_{{\\theta}}({x}_i)) + (1 - y_i)\\log(1 - h_{{\\theta}}({x}_i))]\n",
        "$\n",
        "\n",
        "where $h_\\theta(x)$ is the logistic regression hypothesis function.\n",
        "\n",
        "The gradient of this weighted loss function with respect to the weight $\\theta_j$ is given by:\n",
        "\n",
        "> $\\frac{\\partial \\mathcal{L}({\\theta})}{\\partial \\theta_j} = \\sum_{i=1}^{N}w_{i}(h_{{\\theta}}({x}_i) - y_i)x_{ij}$\n",
        "\n",
        "Using this information, complete the `compute_gradient` method in the `LogisticRegression` class to account for sample weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "    \"\"\"\n",
        "    Logistic Regression (aka logit, MaxEnt) classifier.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha: float, default=0.1\n",
        "        Learning rate\n",
        "    tol : float, default=0.01\n",
        "        Tolerance for stopping criteria\n",
        "    max_iter : int, default=1000\n",
        "        Maximum number of iterations of gradient descent\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    theta_ : numpy.ndarray of shape (D + 1,)\n",
        "        The value of the coefficients after gradient descent has converged\n",
        "        or the number of iterations hit the maximum limit\n",
        "    converged_: boolean\n",
        "        Boolean value indicating whether gradient descent converged or not\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=0.1, tol=0.01, max_iter=1000):\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "        self.theta_ = None\n",
        "        self.converged_ = False\n",
        "\n",
        "    def compute_gradient(self, theta, X, y, sample_weight):\n",
        "        \"\"\"\n",
        "        Compute the gradient of the cost function.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta: numpy.ndarray of shape (D + 1,)\n",
        "            The coefficients\n",
        "        X: numpy.ndarray of shape (N, D + 1)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "        sample_weight: numpy.ndarray of shape (N,)\n",
        "            The sample weight array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        gradient: numpy.ndarray of shape (D + 1,)\n",
        "            The gradient values\n",
        "        \"\"\"\n",
        "\n",
        "        sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
        "        y_hat = sigmoid(X.dot(theta))\n",
        "\n",
        "        # STUDENT TODO: Compute the gradient\n",
        "        ...\n",
        "        \n",
        "        # STUDENT TODO END\n",
        "\n",
        "    def fit(self, X, y, sample_weight):\n",
        "        \"\"\"\n",
        "        Compute the coefficients using gradient descent and store them as theta_.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "        sample_weight: numpy.ndarray of shape (N,)\n",
        "            The sample weight array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Nothing\n",
        "        \"\"\"\n",
        "\n",
        "        N, D = X.shape\n",
        "\n",
        "        # Adding a column of ones at the beginning for the bias term\n",
        "        ones_col = np.ones((N, 1))\n",
        "        X = np.hstack((ones_col, X))\n",
        "\n",
        "        # Initializing the weights\n",
        "        theta_old = np.zeros((D + 1,))\n",
        "        theta_new = theta_old.copy()\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            theta_new = theta_old - self.alpha * self.compute_gradient(theta_old, X, y, sample_weight)\n",
        "\n",
        "            if np.linalg.norm(theta_new - theta_old) / (np.linalg.norm(theta_old) + self.tol) <= self.tol:\n",
        "                self.converged_ = True\n",
        "                break\n",
        "            \n",
        "            theta_old = theta_new.copy()\n",
        "\n",
        "        self.theta_ = theta_new\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict the probabilities that the data points in X belong to class 1.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_hat: numpy.ndarray of shape (N,)\n",
        "            The predicted probabilities that the data points in X belong to class 1\n",
        "        \"\"\"\n",
        "\n",
        "        N = X.shape[0]\n",
        "        \n",
        "        # Adding a column of ones at the beginning for the bias term\n",
        "        ones_col = np.ones((N, 1))\n",
        "        X = np.hstack((ones_col, X))\n",
        "\n",
        "        sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
        "        y_hat = sigmoid(X.dot(self.theta_))\n",
        "        return y_hat\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the classes of the data points in X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred: numpy.ndarray of shape (N,)\n",
        "            The predicted class of the data points in X\n",
        "        \"\"\"\n",
        "\n",
        "        y_hat = self.predict_proba(X)\n",
        "        y_pred = y_hat.copy()\n",
        "        y_pred[y_pred >= 0.5] = 1\n",
        "        y_pred[y_pred < 0.5] = 0\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test case for the `compute_gradient` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_compute_gradient(StudentLogisticRegression):\n",
        "    \n",
        "    student_lr_clf = StudentLogisticRegression()\n",
        "    np.random.seed(0)\n",
        "    theta_tc = np.random.randn(2)\n",
        "    X_tc = np.random.randn(100, 2)\n",
        "    y_tc = np.random.randint(0, 2, 100)\n",
        "    sample_weight_tc = np.random.uniform(0, 1, 100)\n",
        "    student_ans = student_lr_clf.compute_gradient(theta_tc, X_tc, y_tc, sample_weight_tc)\n",
        "    required_ans = np.array([12.903225675830651, -1.0829605960182223])\n",
        "    \n",
        "    assert np.linalg.norm(student_ans - required_ans) < 1e-2 * required_ans.size\n",
        "\n",
        "test_compute_gradient(LogisticRegression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **7.2. [20 pts] AdaBoostClassifier Implementation**\n",
        "\n",
        "In this section, you will be implementing the AdaBoost classifier with the logistic regression weak learner from above.\n",
        "\n",
        "### **7.2.1. [12 pts] Follow the hints in the `fit` method in the `AdaBoostClassifier` class to implement the following algorithm.**\n",
        "\n",
        "Use the following Adaboost pseudocode as a reference.\n",
        "\n",
        "**INPUT:**\n",
        "\n",
        "1. training data $X, y = \\{(x_{i}, y_{i})\\}_{i=1}^N$\n",
        "\n",
        "2. number of iterations $T$\n",
        "\n",
        "**ALGORITHM:**\n",
        "\n",
        "1.   Initialize $N$ uniform weights, i.e., $w_{1} = [1/N, 1/N, ..., 1/N]$\n",
        "\n",
        "2.   `For` $t = 1, 2, ... T$:\n",
        "\n",
        "> 2.1. Train model $h_t$ on $X$ and $y$ with instance weights $w_{t}$\n",
        "\n",
        "> 2.2. Compute the weighted training error rate of $h_{t}$: $\\epsilon_{t} = \\sum_{i: y_i \\ne h_t(x_i)} w_{t,i}$\n",
        "\n",
        "> 2.3. If $\\epsilon_{t} > 0.5$, flip $h_t$'s predictions\n",
        "\n",
        "> 2.4. Set $\\beta_{t} = \\frac{1}{2}\\text{ln}\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)$\n",
        "\n",
        "> 2.5. Update all instance weights: $w_{t + 1,i} = w_{t,i}\\times\\text{exp}(-\\beta_{t}y_{i}h_{t}(x_{i}))$ $\\forall i = 1, 2, ..., N$\n",
        "\n",
        "> 2.6. Normalize $w_{t+1}$ such that the elements sum to 1\n",
        "\n",
        "> `End For`\n",
        "\n",
        "### **7.2.2. [8 pts] Follow the hints in the `predict` method in the `AdaBoostClassifier` class for obtaining the predictions of the trained AdaBoost classifier.**\n",
        "\n",
        "> $H(x) = \\text{sign}\\left(\\sum_{t=1}^{T}\\beta_{t}h_{t}(x)\\right)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdaBoostClassifier:\n",
        "    \"\"\"\n",
        "    AdaBoost classifier based on logistic regression\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    T: int, default=100\n",
        "        The number of logistic regression models in the boosting model\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    beta_arr_ : list of length T\n",
        "        The list of beta values in the boosting model\n",
        "\n",
        "    h_arr_: list of length T\n",
        "        The list of logistic regression models in the boosting model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, T=100):\n",
        "\n",
        "        self.T = T\n",
        "\n",
        "        self.beta_arr_ = []\n",
        "        self.h_arr_ = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the logistic regression models (h) and compute their coefficients (beta), \n",
        "        and store them in h_arr_ and beta_arr_ respectively.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Nothing\n",
        "        \"\"\"\n",
        "\n",
        "        N = X.shape[0]\n",
        "\n",
        "        # STUDENT TODO: Initialize w with appropriate values\n",
        "        ...\n",
        "        # STUDENT TODO END\n",
        "\n",
        "        y_ = y.copy()\n",
        "        # STUDENT TODO: Update y_ such that the 0's in y_ are replaced by -1\n",
        "        ...\n",
        "        # STUDENT TODO END\n",
        "\n",
        "        for t in range(self.T):\n",
        "            h = LogisticRegression()\n",
        "\n",
        "            # STUDENT TODO: \n",
        "            # Fit h to X and y using w as the sample weights\n",
        "\n",
        "            # Obtain the predictions from h and compute epsilon\n",
        "\n",
        "            # If epsilon > 0.5:\n",
        "            # 1. flip the predictions, i.e., replace 1's with 0's and 0's with 1's\n",
        "            # 2. invert the model (h), i.e., make it predict 1 for what it predicted 0 earlier and vice-versa (clue: think about modifying h.theta_)        \n",
        "\n",
        "            # STUDENT TODO END\n",
        "\n",
        "            self.h_arr_.append(h)\n",
        "\n",
        "            if epsilon == 0:\n",
        "                beta = np.inf\n",
        "                self.beta_arr_.append(beta)\n",
        "                break\n",
        "            \n",
        "            # STUDENT TODO: Compute beta\n",
        "            ...\n",
        "            # STUDENT TODO END\n",
        "\n",
        "            self.beta_arr_.append(beta)\n",
        "            y_pred_ = y_pred.copy()\n",
        "\n",
        "            # STUDENT TODO: \n",
        "            # Update y_pred_ such that the 0's in y_pred_ are replaced by -1\n",
        "\n",
        "            # Update w and normalize it such that the values in w sum to 1\n",
        "\n",
        "            # STUDENT TODO END\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the classes of the data points in X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred: numpy.ndarray of shape (N,)\n",
        "            The predicted class of the data points in X\n",
        "        \"\"\"\n",
        "        \n",
        "        N = X.shape[0]\n",
        "        \n",
        "        # Initialize the summation of beta times h for each x_i \n",
        "        sum_beta_times_h = np.zeros((N,))\n",
        "\n",
        "        for t in range(len(self.h_arr_)):\n",
        "            \n",
        "            # STUDENT TODO: \n",
        "            # Obtain the predictions of the t-th model in self.h_arr_\n",
        "\n",
        "            # Replace the 0's in the array with -1\n",
        "\n",
        "            # Update sum_beta_times_h\n",
        "\n",
        "\n",
        "        # Create an array `y_pred` for the final predictions\n",
        "        # Fill 0's and 1's in `y_pred` depending on the sum_beta_time_h value in the corresponding location\n",
        "\n",
        "        return y_pred\n",
        "        # STUDENT TODO END\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test case for the `fit` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_adaboost_fit(StudentAdaBoostClassifier):\n",
        "\n",
        "    T = 4\n",
        "    N = 100\n",
        "    D = 2\n",
        "\n",
        "    student_ab_clf = StudentAdaBoostClassifier(T=T)\n",
        "    np.random.seed(0)\n",
        "    X_tc = np.random.randn(N, D)\n",
        "    y_tc = np.random.randint(0, 2, N)\n",
        "    student_ab_clf.fit(X_tc, y_tc)\n",
        "\n",
        "    beta_arr_student_ans = student_ab_clf.beta_arr_\n",
        "    beta_arr_required_ans = np.array([0.08017132503758954, 0.046732864002838985, \n",
        "                                      0.022808008179707476, 0.07012335626140642])\n",
        "    assert np.linalg.norm(beta_arr_student_ans - beta_arr_required_ans) < 1e-2 * beta_arr_required_ans.size\n",
        "\n",
        "    h_arr_student_ans = np.zeros([T, D + 1])\n",
        "\n",
        "    for indx, h in enumerate(student_ab_clf.h_arr_):\n",
        "        h_arr_student_ans[indx] = h.theta_\n",
        "\n",
        "    h_arr_required_ans = np.array([[-0.01514967, -0.01713051,  0.21344566],\n",
        "                                   [-0.01738886, -0.00656722,  0.12035635],\n",
        "                                   [-0.0132557,  -0.00428943, 0.06616284],\n",
        "                                   [-0.01037174, -0.00334141,  0.03943088]])\n",
        "\n",
        "    assert np.linalg.norm(h_arr_student_ans - h_arr_required_ans) < 1e-2 * h_arr_required_ans.size\n",
        "\n",
        "test_adaboost_fit(AdaBoostClassifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test case for the `predict` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_adaboost_predict(StudentAdaBoostClassifier):\n",
        "\n",
        "    T = 4\n",
        "    N = 100\n",
        "    D = 2\n",
        "\n",
        "    student_ab_clf = StudentAdaBoostClassifier(T=T)\n",
        "    np.random.seed(0)\n",
        "    X_tc = np.random.randn(N, D)\n",
        "    y_tc = np.random.randint(0, 2, N)\n",
        "    student_ab_clf.fit(X_tc, y_tc)\n",
        "\n",
        "    student_ans = student_ab_clf.predict(X_tc)\n",
        "    required_ans = [1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, \n",
        "                    0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, \n",
        "                    1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, \n",
        "                    0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, \n",
        "                    1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1]\n",
        "\n",
        "    assert np.mean(student_ans == required_ans) >= 0.98\n",
        "\n",
        "test_adaboost_predict(AdaBoostClassifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **7.3. [2 pts] AdaBoost on the dataset**\n",
        "\n",
        "Follow the hints in the `adaboost_on_dataset` method in the below cell to run `AdaBoostClassifier` on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv(\"observations_train_data.csv\")\n",
        "train_df = prepare_final_cleaned_df(df)\n",
        "test_df = pd.read_csv(\"observations_test_data.csv\")\n",
        "test_df = prepare_final_cleaned_df(test_df)\n",
        "\n",
        "test_df_label = test_df[\"temp\"]\n",
        "test_df = test_df.drop(columns=[\"temp\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "def adaboost_on_dataset():\n",
        "    \"\"\"\n",
        "    Trains the AdaBoostClassifier on a real-world dataset.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    Nothing\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    y_test_pred: numpy.ndarray\n",
        "        The predicted classes of the datapoints in test_df\n",
        "    \"\"\"\n",
        "\n",
        "    # STUDENT TODO START: Initialize X_train and y_train with appropriate values (clue: use .iloc followed by .values of the DataFrame class)\n",
        "    ...\n",
        "\n",
        "    # STUDENT TODO END\n",
        "\n",
        "    # STUDENT TODO START: Initialize X_test\n",
        "    ...\n",
        "    # STUDENT TODO END\n",
        "    \n",
        "    scaler = StandardScaler()\n",
        "    # STUDENT TODO START: Scale the features of X_train and X_test using scaler\n",
        "    ...\n",
        "\n",
        "    # STUDENT TODO END\n",
        "\n",
        "    clf = AdaBoostClassifier(T=10)\n",
        "    # STUDENT TODO START: Now fit clf to the entire training data, i.e., X_train and y_train after feature scaling\n",
        "    ...\n",
        "    # STUDENT TODO END\n",
        "\n",
        "    # STUDENT TODO START: Predict the classes of the datapoints in X_test and return the result\n",
        "    ...\n",
        "    # STUDENT TODO END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cal_accuracy_student(y_test_pred):\n",
        "    accuracy = np.mean(y_test_pred == test_df_label)\n",
        "    print(\"accuracy: {}\".format(accuracy))\n",
        "    return accuracy\n",
        "\n",
        "def cal_f1_score_student(y_test_pred):\n",
        "    tp = np.sum((y_test_pred == 1) & (test_df_label == 1))\n",
        "    fp = np.sum((y_test_pred == 1) & (test_df_label == 0))\n",
        "    fn = np.sum((y_test_pred == 0) & (test_df_label == 1))\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f1_score = 2 * precision * recall / (precision + recall)\n",
        "    print(\"f1_score: {}\".format(f1_score))\n",
        "    return f1_score\n",
        "y_test_pred = adaboost_on_dataset()\n",
        "acc = cal_accuracy_student(y_test_pred)\n",
        "f1 = cal_f1_score_student(y_test_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **8. [8 pts] XGBoost**\n",
        "\n",
        "## TODOs for this section:\n",
        "- You'll use xgboost library to build a classifier for the above problem. XGBoost is a popular library for gradient boosting, and you can find its documentation [here](https://xgboost.readthedocs.io/en/latest/). \n",
        "- You need to get at least 0.75 accuracy on the test set to receive full credits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv(\"observations_train_data.csv\")\n",
        "train_df = prepare_final_cleaned_df(df)\n",
        "test_df = pd.read_csv(\"observations_test_data.csv\")\n",
        "test_df = prepare_final_cleaned_df(test_df)\n",
        "\n",
        "test_df_label = test_df[\"temp\"]\n",
        "test_df = test_df.drop(columns=[\"temp\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# STUDENT TODO STARTS: \n",
        "# 1. Fit an xgboost classifier to the training data (train_df, target variable is temp) \n",
        "# 2. Obtain the predictions of the trained model on test_df in the variable y_test_pred\n",
        "# 3. Tune the hyperparameters such that you pass the autograder threshold accuracy of 0.75\n",
        "...\n",
        "\n",
        "# STUDENT TODO ENDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc = cal_accuracy_student(y_test_pred)\n",
        "f1 = cal_f1_score_student(y_test_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **9. [25 pts] K-means Clustering**\n",
        "\n",
        "We will implement the k-means clustering algorithm using the Breast Cancer dataset. As with all unsupervised learning problems, our goal is to discover and describe some hidden structure in unlabeled data. The k-means algorithm, in particular, attempts to determine how to separate the data into <em>k</em> distinct groups over a set of features ***given that we know (are provided) the value of k***.\n",
        "\n",
        "Knowing there are <em>k</em> distinct 'classes' however, doesn't tell anything about the content/properties within each class. If we could find samples that are representative of each of these *k* groups, then we could label the rest of the data based on how similar they are to each of the prototypical samples. We will refer to these representatives as the centroids (cluster centers) that correspond to each cluster.\n",
        "\n",
        "## **9.1. Import the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "cancer_dataset = load_breast_cancer()\n",
        "\n",
        "## TODO your code here ##\n",
        "\"\"\"\n",
        "First load the dataset X from cancer_dataset.\n",
        "X -  (m, n) -> m x n matrix where m is the number of training points = 569 and n is the no of features = 30\n",
        "\"\"\"\n",
        "...\n",
        "## TODO end ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **9.2. [12 pts] K-means clustering implementation** \n",
        "\n",
        "We will first implement a class for k-means clustering.<br>\n",
        "These are the main functions: <br>\n",
        "- `__init__`: The initialiser/constructor (This is implemented for you)\n",
        "- `fit`: Entrypoint function that takes in the dataset (X) as well as centroid initialisations and returns: \n",
        "    - the cluster labels for each row (data point) in the dataset\n",
        "    - list of centroids corresponding to each cluster \n",
        "    - no of iterations taken to converge.\n",
        "\n",
        "Inside fit() function, you will need to implement the actual kmeans functionality. <br>\n",
        "The K-means process you should follow is listed below:\n",
        "1. Initialize each of the k centroids to a random datapoint if initialisation is not provided.\n",
        "2. Update each datapoint's cluster to that whose *centroid* is closest\n",
        "3. Calculate the new *centroid* of each cluster\n",
        "4. Repeat the previous two steps until no centroid value changes. Make sure you break out of the loop reagrdless of whether you converged or not once max iterations are reached.\n",
        "\n",
        "To help streamline this process, three helper functions have been given to you in the KMeans class \\\n",
        "- compute_distance(): use for step-2 above\n",
        "- find_closest_cluster(): use for step-2 above\n",
        "- compute_centroid(): use for step-3 above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KMeans:\n",
        "    '''Implementing Kmeans clustering'''\n",
        "\n",
        "    def __init__(self, n_clusters, max_iter=1000):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def compute_centroids(self, X, clusters):\n",
        "        \"\"\"\n",
        "        Computes new centroids positions given the clusters\n",
        "        \n",
        "        INPUT:\n",
        "        X - m by n matrix, where m is the number of training points\n",
        "        clusters -  m dimensional vector, where m is the number of training points \n",
        "                    At an index i, it contains the cluster id that the i-th datapoint \n",
        "                    in X belongs to.\n",
        "        \n",
        "        OUTPUT:\n",
        "        centroids - k by n matrix, where k is the number of clusters.\n",
        "        \"\"\"\n",
        "        centroids = np.zeros((self.n_clusters, X.shape[1]))\n",
        "        # TODO your code here\n",
        "        ...\n",
        "\n",
        "        ## TODO end ##\n",
        "        return centroids\n",
        "\n",
        "    def compute_distance(self, X, centroids):\n",
        "        \"\"\"\n",
        "        Computes the distance of each datapoint in X from the centroids of all the clusters\n",
        "        \n",
        "        INPUT:\n",
        "        X - m by n matrix, where m is the number of training points\n",
        "        centroids - k by n matrix, where k is the number of clusters\n",
        "        \n",
        "        OUTPUT:\n",
        "        dist - m by k matrix, for each datapoint in X, the distances from all the k cluster centroids.\n",
        "        \n",
        "        \"\"\"\n",
        "        dist = np.zeros((X.shape[0], self.n_clusters))\n",
        "        # TODO your code here\n",
        "        ...\n",
        "\n",
        "        ## TODO end ##\n",
        "        return dist\n",
        "\n",
        "    def find_closest_cluster(self, dist):\n",
        "        \"\"\"\n",
        "        Finds the cluster id that each datapoint in X belongs to\n",
        "        \n",
        "        INPUT:\n",
        "        dist - m by k matrix, for each datapoint in X, the distances from all the k cluster centroids.\n",
        "        \n",
        "        OUTPUT:\n",
        "        clusters - m dimensional vector, where m is the number of training points \n",
        "                    At an index i, it contains the cluster id that the i-th datapoint \n",
        "                    in X belongs to.\n",
        "        \n",
        "        \"\"\"\n",
        "        clusters = np.zeros(dist.shape[0])\n",
        "        # TODO your code here\n",
        "        ...\n",
        "        ## TODO end ##\n",
        "        return clusters\n",
        "    \n",
        "    def fit(self, X, init_centroids=None):\n",
        "        \"\"\"\n",
        "        Fit KMeans clustering to given dataset X. \n",
        "        \n",
        "        INPUT:\n",
        "        X - m by n matrix, where m is the number of training points\n",
        "        init_centroids (optional) - k by n matrix, where k is the number of clusters\n",
        "        \n",
        "        OUTPUT:\n",
        "        clusters - m dimensional vector, where m is the number of training points \n",
        "                    At an index i, it contains the cluster id that the i-th datapoint \n",
        "                    in X belongs to.\n",
        "        centroids - k by n matrix, where k is the number of clusters. \n",
        "                    These are the k cluster centroids, for cluster ids 0 to k-1\n",
        "        iters_taken - total iterations taken to converge. Should not be more than max_iter.\n",
        "        \n",
        "        \"\"\"\n",
        "        # Fix random seed. Do not change this!\n",
        "        np.random.RandomState(111)\n",
        "\n",
        "        ## TODO your code here ##\n",
        "        # Initialise centroids to random points in the dataset if not provided (i.e. None)\n",
        "        ...\n",
        "\n",
        "        # Iterate until kmeans converges or max_iters is reached. In each iteration: \n",
        "        #  - Update each datapoint's cluster to that whose *centroid* is closest\n",
        "        #  - Calculate the new *centroid* of each cluster\n",
        "        #  - Repeat the previous two steps until no centroid value changes. \n",
        "\n",
        "        ...\n",
        "        ## TODO end ## "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test case centroids should be aroudn (1.5,1.5) and (4.5,4.5)\n",
        "points = []\n",
        "result = []\n",
        "random.seed(0)\n",
        "for _ in range(500):\n",
        "    x = random.random()*3\n",
        "    y = random.random()*3\n",
        "    points.append((x,y))\n",
        "    result.append(0)\n",
        "for _ in range(500):\n",
        "    x = random.random()*3 + 3\n",
        "    y = random.random()*3 + 3\n",
        "    points.append((x,y))\n",
        "    result.append(1)\n",
        "clf = KMeans(2)\n",
        "points = np.asarray(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#test for sanity check\n",
        "def test_compute_centroids():\n",
        "  clf = KMeans(2)\n",
        "  centroid_p = clf.compute_centroids(np.array(points),np.array(result))\n",
        "  centroid_r = [[1.5185255, 1.45970038],\n",
        " [4.51568108,4.54138552]]\n",
        "  assert(np.linalg.norm(centroid_p - np.array(centroid_r)) <= 1e-2 )\n",
        "test_compute_centroids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_distance():\n",
        "    centroid_r = [[1.5185255, 1.45970038],\n",
        "      [4.51568108,4.54138552]]\n",
        "    clf = KMeans(2)\n",
        "    distance = clf.compute_distance(np.array(points),np.array(centroid_r))\n",
        "    distance_for_0 = [1.30098366, 3.01191447]\n",
        "    assert(np.linalg.norm(distance_for_0-distance[0]) <= 1e-2)\n",
        "test_distance()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_find_clusters():\n",
        "  centroid_r = [[1.5185255, 1.45970038],\n",
        "      [4.51568108,4.54138552]]\n",
        "  clf = KMeans(2)\n",
        "  distance = clf.compute_distance(np.array(points),np.array(centroid_r))\n",
        "  cluster = clf.find_closest_cluster(distance)\n",
        "  assert(cluster[0] == 0)\n",
        "test_find_clusters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_fit():\n",
        "  clf = KMeans(2)\n",
        "  clusters, centroids, _ = clf.fit(np.array(points),np.array([[1,1],[4,4]]))\n",
        "  centroid_r = [[1.5185255, 1.45970038],\n",
        "      [4.51568108,4.54138552]]\n",
        "  assert(np.linalg.norm(centroids - np.array(centroid_r)) <= 1e-2 )\n",
        "  assert(sum(np.array(clusters)-np.array(result)) == 0)\n",
        "test_fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **9.3. [3 pts] Compute distortion**\n",
        "\n",
        "As you may have noticed already, one big question still remains. How do we know what value of k to choose?\n",
        "\n",
        "One way to decide on a value for k is to run k-means and plot the distortion (sum of squared error based on the Euclidean distance). From that we can find the \"elbow of the graph\" that indicates the best tradeoff between number of clusters and corresponding distortion.\n",
        "\n",
        "In the function `test_cluster_size`,  iterate over possible cluster sizes from 2 to a `max_cluster` (inclusive) value. For each *k*, run k-means and calculate its distortion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_cluster_size(X, max_k):\n",
        "    \"\"\"\n",
        "    Iterates over possible cluster from 2 to max_k, running k-means and calulating distortion.\n",
        "    \n",
        "    INPUT:\n",
        "    X - m by n matrix, where m is the number of training points\n",
        "    max_k - the maximum number of clusters to consider\n",
        "    \n",
        "    OUTPUT:\n",
        "    scores - a list of scores, that contains the distortion for k = 2 to max_k, in order.\n",
        "    \"\"\"\n",
        "    scores = [0] * (max_k-1)\n",
        "    ## TODO your code here ##\n",
        "    ...\n",
        "    \n",
        "    ## TODO end ##\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_test_cluster_size():\n",
        "  scores = test_cluster_size(np.array(points),5)\n",
        "  assert(np.argmax(scores) == 0)\n",
        "test_test_cluster_size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **9.4. [3 pts] Plot distortion vs. k (without feature scaling)** \n",
        "\n",
        "Plot distortion vs. different k values by using the function we just wrote on dataset X and add it in the written report. Use max_k = 20. Determine the best k value from this plot and also mention it in the written report. Make sure your plot has axes labels, legend and title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO your code here ##\n",
        "...\n",
        "## TODO end ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **9.5. [3 pts] Plot distortion vs. k (with feature scaling)** \n",
        "\n",
        "What we just did was running k-means clustering over the dataset X without any feature scaling. This time, we will rescale each feature to the standard range of (0,1) before passing it to k-means and computing the distortion.\n",
        "\n",
        "Use `sklearn.preprocessing.MinMaxScaler` ([docs](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)) and scale the dataset X before passing it to the `test_cluster_size` function. As before, plot distortion vs. different k values and add it in the written report. Use max_k = 20. Determine the best k value from this plot and also mention it in the written report. Make sure your plot has axes labels, legend and title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO your code here ##\n",
        "# Use min-max scaler to scale the dataset\n",
        "...\n",
        "\n",
        "## TODO end ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **9.6. [4 pts] Comments**\n",
        "\n",
        "Answer these questions in the written report.\n",
        "\n",
        "1. Why do you get different results with and without feature scaling?\n",
        "2. Should you scale the features before fitting k-means? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **10. [18 pts] Principal Component Analysis** \n",
        "\n",
        "## **10.1. [8 pts] Exploring Effects of Different Princple Components in Linear Regression**\n",
        "We have introduced you a way of dimension reduction, Principal Component Analysis, in class. Now, we would like to ask you to apply PCA from sklearn on the breast cancer dataset to observe its performance and interpret the major components.\n",
        "\n",
        "In order to better compare the effects of PCA, we load the labels from the dataset.\n",
        "\n",
        "Then, we will evaluate the performances of raw dataset and various numbers of pca components on LinearRegression classifier.\n",
        "\n",
        "In the section, you are asked to draw a plot of test accuracies vs number of different principle components. The detailed instructions are included in the following cells.\n",
        "\n",
        "Remember to **attach the plot** in your written submission, and also **make comments** about what you observe, explain the reason behind the trend, and what conclusion you could draw from the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the label from the dataset, which is a binary label 0/1 representing whether the cancer is benign or malignant\n",
        "\n",
        "## TODO your code here ##\n",
        "...\n",
        "## TODO end ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# try raw data vs PCA data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "## TODO your code here ##\n",
        "# Step 1: split the data into train and test set by a test_size of 0.33.\n",
        "...\n",
        "\n",
        "# Step 2: Train a linear regression model using train set and predict on the test set.\n",
        "# As the labels are binary, we should cast the predictions into binary labels as well. (Set predictions >=0.5 as 1)\n",
        "\n",
        "# You might want to print out accuracy scores here\n",
        "print(\"Raw data has accuracy: \", raw_accu)\n",
        "\n",
        "\n",
        "# Step 3: Iterate the number of components from 1 to 10 (exclusive). \n",
        "# For each number of PCs, we are training a linear regression model and save its accuracy on the test set following the same style as above.\n",
        "# Remeber to only fit the train set and not the test set. \n",
        "# You might want to store your accuracies in a list\n",
        "accuracies = []\n",
        "\n",
        "...\n",
        "\n",
        "# Step 4: Make a plot to compare accuracy vs number of PCs on Linear Regression for the test set.\n",
        "# Add a black, dashed line for the test accuracy of linear regression by feeding the raw input data.\n",
        "# Remeber to add x, y labels and title to your plot, and comment on your observations.\n",
        "...\n",
        "\n",
        "## TODO end ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **10.2. [5 pts] Understanding PCA**\n",
        "\n",
        "### **10.2.1 [2 pts] Explained Ratio of PCA**\n",
        "Given a threshold of explained ratio (0 < ratio < 1), compute the number of required PCs to reach the threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_n_principal_components(data, variation):\n",
        "  ## TODO your code here ##\n",
        "  ...\n",
        "\n",
        "  ## TODO end ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **10.2.2 [3 pts] Composition of PCA**\n",
        "In this section, we ask you to understand which features specifically in the dataset contribute to the important PCs. We ask that you select the best number of principle components you got from previous part and analyze its composition (as there are multiple components contributing to each PC, you only need to specify the **top three** features that are explained by these PCs together)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO your code here ##\n",
        "...\n",
        "\n",
        "## TODO end ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **10.3. [5 pts] PCA and KMeans**\n",
        "We first run PCA on the dataset for visualisation in 2D space. Note that k-means is actually being fit on the entire feature set. \n",
        "\n",
        "Next, call your k-means class on the dataset X and obtain the clusters. Make sure to populate the \"clusters\" variable here. We have provided the plotting code for you.\n",
        "\n",
        "**Add these plots in the written report.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PCA for visualisation in 2D. \n",
        "pca = PCA(n_components = 2)\n",
        "v = pca.fit(np.transpose(X)).components_\n",
        "\n",
        "for k in [3,5,7,9, 11]:\n",
        "\n",
        "    clusters = np.zeros(X.shape[0])\n",
        "\n",
        "    ## TODO your code here ##\n",
        "    ...\n",
        "\n",
        "    ## TODO end ##\n",
        "\n",
        "    plt.scatter(v[0], v[1], c=clusters, s=18)\n",
        "    plt.title(\"Breast Cancer Clusters (k = \"+str(k) + \")\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFPkrjyp5vub"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOT4XbEE6MKF"
      },
      "source": [
        "- Submit the notebook as a `homework-1111111111-.ipynb` file named with your student number and name to the coding portion of the Lexue platform."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
